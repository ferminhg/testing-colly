[{"id":"5fad941c-e001-4b70-8ea9-3443f05be5f7","title":"A Guide to Threat Modelling for Developers","link":{"Scheme":"https","Opaque":"","User":null,"Host":"martinfowler.com","Path":"/articles/agile-threat-modelling.html","RawPath":"","OmitHost":false,"ForceQuery":false,"RawQuery":"","Fragment":"","RawFragment":""},"text":"\n\nHow to simplify a complex problem\n\nWhat are the security requirements for the software you are building?\n  Finding a good answer is surprisingly complex. You want to prevent cyber\n   losses over the lifetime of the system. \n  But what are the concrete stories, acceptance criteria and technical scope that\n  delivers that outcome? That is the puzzle addressed in this guide. \n  \n\n\n-- https://twitter.com/dakami/status/1249487040858578945\n\n\nSomewhat unhelpfully, cyber specialists will often ask: \n  'What is your threat model?' This answer is very non-specific and uncertain, \n  much like turning around and saying 'it depends'.  Worse, 'threat model' is obscure \n   technical jargon for most people adding unnecessary mystique. \n  And if you research the topic of threat modelling the information can be overwhelming and hard to action. \n  There is no agreed standard for a 'threat model' or anything like that.\n\nSo what are threat models and what is threat modelling? The core of the concept is very simple. It is about understanding\n  causes in relation to cyber security losses. It is about using that understanding \n  to protect your system in a risk-based way. It means starting from the \n  potential threats in your particular case, rather than just following a checklist.\n\n\n\n\n\n\n\n     The reality of threats is that many causes combine. Cyber threats chain in unexpected, unpredictable and even chaotic ways. \n     \n\n\nComing to understand the threat model for your system is not simple. There are an unlimited number of threats \n  you can imagine to any system, and many of them could be likely. The reality of threats is that many causes combine. Cyber\n  threats chain in unexpected, unpredictable and even chaotic ways. Factors to do with culture, process and technology \n  all contribute. This complexity and uncertainty is at the root\n  of the cyber security problem. This is why security requirements are so hard for software development teams to agree\n  upon.\n\nThe stories behind real breaches show how complex threats and causality can be- often the details are astounding.\n  The NotPetya story is a great example. Nation state malware was traded\n  by a group called the \"ShadowBrokers\" and then weaponised. The eventual impact was major losses to organisations\n  almost at random. Mearsk, the shipping firm, had to halt the progress of shipping. The confectioner Cadbury's\n  had to stop making chocolate. What were their respective threat models? What development team could imagine such \n  a complex chain of causality and collatoral damage? How long would it take your team to model this, and every other \n  dangerous possibility?\n\nIs threat modelling too complex to be of value? Should developers just follow a checklist, 'cross their fingers'\n  and hope they get lucky? Skepticism can be healthy, but learning threat modelling is a key \n  skill for developers, I believe. What we need\n  is the right approach, and tools to tame the complexity. This guide has been written in that spirit, and begins with\n  three ideas which make identifying good, risk-based security requirements much simpler.\n\n\nStart from the technology\n\nThe first recommendation is to focus primarily on technical rather than broad threats, at least at first.\n\n\nBroad threats and threat sources include hacker groups, bad actors, disillusioned employees, human error or epidemics of new worm-like malware. These kinds \n  of causes emerge from the world at large and are extremely various, uncertain and unpredictable. They are relative to the value \n  of your system's data and services to your organisation and to others.  These are the kinds of dramatic risks it is easy\n  to talk about with non-technical folks.\n\n\nRabbit hole: What about nation states and 0-day?\n\n\n\n\n\nIn my experience, some developers enjoy talking about nation state adversaries, with mysterious capabilities \n    such as 0-day exploits. Even quantum computers! Remember, thinking about risk is thinking about what is likely. Nation states are \n    not the main risk for most systems. If you believe your system is an exception to this, I recommend escalating the threat within of your organisation \n    to get the support and help you need with security.\n\n\nTechnical threats and vulnerabilities are much more granular, such as particular weaknesses in software or missing\n  security controls such as encryption or authorisation. These kinds of threats emerge from the structure and data-flow inherent in\n  the system your team is building. Usually a bunch of technical threats combine together to allow a broad threat to impact your system.\n\n\nBy following this guide you will mainly focus on finding technical threats. This helps simplify the elaboration process, because the \n  structure and data-flow of your system is something about which you can be certain. But it also means you can start from your existing strengths as a software\n  developer, understanding technical stuff. This is a much stronger ground to start on than high-level risk analysis of threat sources, about which \n  you may know little. \n  \n\nDon't forget about the bigger picture entirely though. A pragmatic and risk-based understanding of what \n  broad threats are possible helps prioritise one technical threat over another. For example simple human error is usually much more likely\n  than a nation state attack (see sidebar). That thinking can go into selecting what security scope to start examining first. When you focus on identifying \n  technical threats first, it is then much easier to relate them back to broader threats that justify fixes and additional controls.\n\n\n\nTake a collaborative approach\n\nThe second recommendation is to adopt a collaborative, team based approach. Identifying security requirements is not easy, and \n  a diversity of perspectives will lead to better decision making. There will always be \n  another vulnerability or technical threat to find, so bringing a wide variety of perspectives to the exercise makes brainstorming more robust. \n  It also increases the likelihood you will identify the most important threats. Threat modelling in a group helps address risk holistically\n  and helps the whole team to learn how to think and talk effectively about security.\n\n\nRabbit hole: Building the perfect threat model\n\n\n\n\n\nOften developers seek some perfect final artifact:\n      the definitive 'threat model'. This might take the form of a document or spreadsheet detailing\n      the whole system alongside an exhaustive list of threats, vulnerabilities and attack \n      trees.\n\nThe first problem with this, is that your system changes and you will never find all the \n      threats. The second problem is that producing a separate\n      artifact doesn't help with figuring out what steps are needed to combat the\n      the threats you have found.\n\nSee how far you can get by following the steps in this guide, before you embark on \n      building an elaborate and formal 'threat model'.\n\n\nGetting product owners involved is a great oppotunity from a risk management perspective.\n  Product owners have insights into user behaviour and business context that software developers simply lack. They should \n  know about the value of particular services to the business and the impact if that data was exposed or lost. When cyber security losses \n  occur they are business losses. If the worst does happen then the causes will likely be particular to your organisation and the \n  technology you are using. The cyber security problem is not just about ticking technical boxes, its about making good investment \n  decisions to protect the business.\n\n\n\nThreat modelling 'little and often'\n\nThe third recommendation is to start threat modelling 'little and often'. Each threat modelling session\n  with the team should be short and focussed \n  enough to be quickly digested into something that can be delivered. Start by analysing the thinnest slice of your \n  system possible; just what you are working on right now. Rather than trying to analyse your entire system upfront, \n  build your team's muscle memory with threat modelling a little bit at a time.\n\nPractises which require a completely specified software design do not match how agile teams work. There is no reason why\n  threat modelling needs to be an exhaustive upfront analysis. Too often teams are overwhelmed by \n  comprehensive and highly structured approaches to threat modelling[1]. I have seen teams try \n  such approaches and run out of time and patience before any real threats were identified- let alone fixes delivered!\n\nRather than creating and maintaining an exhaustive 'threat model' document, do threat modelling 'little and often'. \n   When you work this way each threat modelling session is tiny, having little impact. Yet the cumulative effect of doing them \n   has a huge impact. When you know you'll be doing this again every iteration, there's less incentive to try to do everything \n   at once and more incentive to prioritize the most important work right now.\n\n\n\n\nPreparing to start\n\nThis section of the guide starts to make things more detailed and concrete so you can \n        plan to start threat modelling with your team.\n\n\nThe three key questions\n\n\nUnderstanding the simple structure of a \n         threat modelling session, and doing a little bit of planning goes a long way to getting a \n         great result.\n\n\nThe first thing to\n         introduce is a simple and flexible structure for threat modelling [2].\n         This is based on three key questions. It helps to commit this structure to memory. You can use the three\n         question structure as a guide whenever you need to assess threats.\n         Like riding a bike, once \n        you have mastered the basics you will be able to apply and grow those skills. \n         \n\n\nActivityQuestionOutcome\n\nExplain and exploreWhat are you building?A technical diagram\n\nBrainstorm threatsWhat can go wrong?A list of technical threats\n\nPrioritise and fixWhat are you going to do?Priorised fixes added to backlog\n\n\nThis guide follows the three question structure. In each threat modelling session, you should spend about a third of your\n    time answering each question. Then you will come out with a useful result. The rest of the guide will\n    break this basic structure into more detailed steps, pointers and explanations to help you run \n    successful threat modelling sessions.\n\n\n\nPractical considerations\n\nThere are some things you need to get straight before you run a threat modelling session. The following \n    pointers should help you plan.\n\n\nWho should be involved?\n\n\n\n\n\n\nIf you are a security specialist, these sessions help you too. They provide the ideal \n      forum to collaborate and learn about \n    the deep technical structure of the systems the development teams are building. Also it is \n    the perfect chance to share key insights with development teams around\n    risk and security architecture.\n\n\nTry to involve the whole delivery team in each session, which is to say both technical and non-technical roles. This brings more perspectives and \n    ideas to the table, but also builds shared understanding. Excluding product owners, business analysts and delivery\n    managers can mean the work to fix security flaws does not get done, as the value will not be widely \n    understood.\n\nYou definitely do not need a security specialist to start threat modelling and discover valuable security scope. \n    However, a threat modelling session is the perfect opportunity to collaborate with specialists,\n    security architects or your risk management team. It will all depend on what the roles and expertise \n    are like in your organisation.\n\n\n\nCadance and duration\n\nTo start, I recommmend a session length of 90 minutes. You need to give the team the time and space \n    to learn the structure and security concepts involved. Things should get much faster once you get going,\n    though. The most impactful threat modelling session I have ever participated in took less than 15 minutes.\n    Short and snappy sessions are possible once everyone in the team has built 'muscle memory' with the practise.\n\nI am often asked how frequent threat modelling sessions should be. I do not think there is any right answer,\n    it depends on your team. I think of threat modelling just\n    like any other team design session. I would not be so rigid as saying it has to be every single week. \n    However, I have worked with many teams with a risk profile that would justify threat modelling every sprint.\n    At the other extreme if it has been a few sprints without any threat modelling, the practise is clearly \n    not continuous enough to be considered mature.\n\n\n\nRunning sessions face-to-face vs. running remotely\n\n\n\n\n\n\nHere's what you need to prepare for a face-to-face session:\n\n\nWhiteboard or flipchart\n\nColoured pens for diagrams\n\nSticky notes for everyone\n\nSharpie pens for everyone\n\n\nIf you are running remotely, in addition to a video conferencing tool, find a collaboration tool which offers \n    the following:\n\n\nCollaborative editing\n\nCreate, modify and delete cards/labels\n\nDraw block diagrams\n\nUse different colours\n\nAllow participants to vote\n\n\n\nA face-to-face threat modelling session could happen in a meeting room, or more informally \n    in the team's normal work area- if you have space. A typical session involves drawing a diagram to explain \n    and explore the scope, brainstorming threats, then prioritising fixes for the backlog. However, a face-to-face \n    session is not always possible.\n\nWhen you run a session remotely you just need to plan a little differently so everyone can \n    participate virtually. You will need video conferencing and collaboration tools. Agree and setup\n    these tools ahead of time. Teams at Thoughtworks have had success with \n    a variety of tools, including Mural, Miro, Google Jamboard and Google Docs.\n\nGet accustomed to your tools ahead of the session and get participants to test they have access. Whichever \n    tool you choose, ensure you have security approval from your organisation to use the tool. \n    Threat modelling outputs represent sensitive information for a number of reasons and must \n    be protected.\n\nHere are some more pointers to bear in mind when working remotely:\n\n\nIt can help to create diagrams asyncronously before the exercise. This is \n      because it can consumes a good amount of time to draw diagrams on virtual boards.\n\nPay even more attention to creating a common understanding of the concepts and symbols you\n    are using to illustrate the system. Explain diagram symbols, data flow arrows and colours of digital stickies.\n    \n\nBe more intentional to ensure everyone is engaged in the exercise. \n    Perhaps use some security related brain teasers as an ice-breaker. Refer to broader guides \n    on remote facilitation.\n\nIf you have a large group of people, it may make sense to create smaller groups \n     and then consolidate the output. A couple of small sessions is better and more sustainable \n     than one big session. \n\nYou will need more breaks than required in a face-to-face session. Remote work is tiring.\n\n\nRegardless if your session is remote or face-to-face, you should aim to finish on time.\n      And with some concrete outcomes! This needs discipline - facilitating \n     timings could be a role taken by a delivery manager or someone experienced making sure workshop sessions succeed.\n    \n\n\nMona Fenzl and Sarah Schmid from Thoughtworks Germany have \n    had some success using a collaboration tool called Mural. They used it to create a\n    threat modelling template to help other teams get started with that tool.\n\n\n\n\n\nScoping the session\n\n\n        \n        Deciding the right focus and level of detail for your session is called \n        'identifying the scope'. Make sure you have decided this ahead of getting people \n        together to perform the activity. Be guided by what has most value right now. Perhaps\n        its simply the user stories you are working on this iteration?\n\nDo not try and bite off too much scope at once! If you try threat modelling the entire \n        system at once, either you will make no findings in the time available or \n        you will overrun dramatically and there will be no appetite or budget to do it again.\n        It is much better to timebox threat modelling into \n        manageable chunks, performing the activity 'little and often'.\n\nHere are some examples of scopes which have worked well:\n\n\nScope in current iteration.\n\nAn upcoming security sensitive feature, such as a new user registration flow.\n\nThe continuous delivery pipeline and delivery infrastructure.\n\nA particular microservice and its collaborating services\n\nA high level overview of a system to identify security tech debt.\n\n\nWhatever scope your team chooses, make sure it is not too big for you to cover in the time available.\n\n\n\n\nA worked example: scoping\n\nThe rest of the guide uses a real feature to show the \n        concrete steps involved in threat modelling. There is a development team at a retail \n        organisation which is building a platform to sell groceries for home delivery. Here is the \n        epic they have in the upcoming sprint:\n\n\n\n        As a customer,\n        I need a page where I can see my customer details,\n        So that I can confirm they are correct\n        \n\n\nIf you have ever used an online store, you will be able to imagine a page which is used to update\n        address details and perhaps view a loyalty card balance.\n\n From experience, a feature of this size is a\n        pretty reasonable scope for threat modelling session.\n\n\n\n\n\n\n\nExplain and explore\n\n\n\n         Do not be tempted to pull a stale image off the Wiki. Talking through how the \n         system is right now- or will be soon, builds shared understanding.\n\n\n\"What are you building?\"\n\n Diagrams are the perfect tool to explain \n         and explore how software is structured, and designed to communicate. This section of the guide \n        provides detailed pointers on the diagram which will serve as the foundation \n        of your threat modelling session.\n\n\nDraw a 'lo-fi' technical diagram\n\nDrawing a picture gets everyone on the same page. Before you can start thinking of threats, risk\n         and mitigations you need a shared technical \n         understanding of the software or infrastructure you are dealing with.\n\n\n\ni. Show relevant components.\n\n Luckily, developers will be comfortable drawing diagrams to explore software\n            designs. Tap into these established skills by drawing a simple technical diagram of your agreed scope on the whiteboard\n            or flipchart.\n\nNothing needs to be sophisticated or perfect - just draw boxes for the main components and label them.\n\n\n\nii. Show users on the diagram.\n\n Ultimately, systems are designed to allow people to do things. Users matter as they are the ones authorised to things\n            in the system. Represent and label them on your diagram.\n\n\nSome users can be more trusted than others. For example end-users usually have less freedom to perform operations \n            in a system than administrators. If multiple groups of users are relevant to the scope of the session, represent and \n            label each group.\n\nNot all systems are user facing. If your system is backend system (perhaps a downstream microservice which only \n            accepts requests from other systems) then represent collaborating systems that are authorised to interact with \n            the system.\n\n\nTrust is essentially about who or what should have the freedom to do a particular thing. Make sure you illustrate those \n             'actors', as they are important for security.\n\n\n\n\n\n\nA worked example: Lo-fi diagram\n\nReturning to the real feature introduced above, lets see how the team chose to illustrate the new \n          'customer details page' functionality. They drew the following diagram.\n\n\n\n\n\n\n\nIt illustrates an understanding that the system:\n\n\nis based on a microservice architecture\n\nalready has an identity provider in place which allows the customer to authenticate.\n\nhas a backend service for customer details (which is written in Java)\n\nhas a Backend for Frontend (BFF) service and frontend UI (which are written in Javascript and React)\n\nhas users who are customers, and want to edit their profile\n\n\nNo detailed knowledge of these technologies is required to follow this guide, but these facts illustrate the level of detail you should discuss while \n        drawing the diagram.\n\n\n\n\n\n\nShow how data flows\n\nIt is important to add details to show how data flows around your system.\n\n\n\niii. Draw arrows to show data-flow\n\nAttackers often use the same pathways to pivot around systems that\n            legitimate users do. The difference is they abuse them or use them in ways that nobody thought of \n            checking. So it is important to show the trusted paths around your system, to helps see where \n            real threats could happen. \n            \n\nShow data-flows with arrows, starting from the users and collaborating systems. Nowadays, most \n            data-flows are request-response and therefore bi-directional. But I recommend that you draw directional\n            arrows from where requests originate. From experience this makes it much easier to brainstorm threats later on.\n            \n\n\n\niv. Label networks and show boundaries.\n\nIt is more likely threats will originate from certain networks. \n            Networks are configured to restrict the freedom of traffic to flow from one place to another. That restrictiveness (or openness)\n            will help determine what threats are possible or likely. The open Internet \n            is more dangerous than a well protected backend network (even if your backend network is a VPC hosted by your cloud provider). \n\nIn another colour, draw dotted lines to show the boundaries between different networks in your system. These are often \n            called 'authorisation boundaries'. Sometimes it is \n            worth illustrating gateways devices such as load balancers or firewalls. Other times those devices are not so\n            important to your scope in that session, and that is okay too. If you are unsure, it might make sense to invite someone \n            with DevOps or infrastructure knowledge to your next session.\n\n\n\n\n\n\nA worked example: Show Data-flow\n\n\n\n\n\n\n\nIn our case arrows are added to illustrate data-flows from the customer who wants to see their\n        details, to the UI, then on to the BFF service and onto the customer details resource server to obtain \n        or update the data. There is also a data-flow to the Identity server which issues a token that authorises the session.\n\nThere is also an authorisation boundary because the UI is on the Internet, whereas the other components are within \n        the organisation's cloud hosting.\n\n\n\n\n\n\nv. Show your assets.\n\nIt is helpful to quickly indicate on the diagram where data or services with business value sit. For example, \n            this may be where you store personal data. If your system processes payments, perhaps its the service which does \n            that. The assets in your system are information that needs to be kept confidential or intact, but also services which \n            need to be kept available.\n\nDo not spend too long on this step. The purpose is just to provide a little bit of context to help with \n            brainstorming and prioritisation. If you spend more than 5 minutes on this, then its probably too long.\n\n\n\n\n\n\nA worked example: Show your assets\n\nThe team identified the personally identifable information (PII) stored by the Customer Service \n        and the credential store in the Identity provider as the assets with most business value.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrainstorm threats\n\n\"What can go wrong?\"\n\n\n       For the second part of your session, simply brainstorm threats to the\n       system you have drawn.\n      This section provides detailed steps and pointers to help you come up \n      with a good range of relevant security threats.\n\n\n-- https://twitter.com/shehackspurple/status/1097034074177232897\n\n\n\nUse STRIDE to help\n\nIf your team is beginning with threat modelling, STRIDE is perfect.\n        STRIDE is a very light framework that gives you a head-start brainstorming security threats.\n        It is a mnemonic, where each letter refers to a security concept. The point is not about\n        categorising what you find, but helping you brainstorm effectively.\n\nInvest some time understanding and discussing each of the six security\n         concepts with the team before you start. To help learn, Thoughtworks has created a\n         set of STRIDE cue cards. These six cards are linked immediately below. They also include \n         lists of examples on the reverse side.\n\n\n\nOn the Internet, nobody knows you are a dog\n\n\n\nCan the data overflow and become instructions?\n\n\n\nIf there's no evidence, its easy to deny it happened\n\n\n\nWho else could be looking?\n\n\n\nCould the service be taken down?\n\n\n\nHow easy is it to circumvent protections?\n\n\n\n\n\nEvil brainstorming\n\nComing up with ways to attack, break or frustrate\n      a particular bit of software is threat modelling at its essence. It can also be great fun!\n\n\nRabbit hole: Wrangling over suggestions\n\n\n      It is common to get sucked into debates when folks suggest threats. Usually someone will suggest \n      a threat and someone else says \"Yeah but...\"\n\n\n      Yeah but ... does  encryption really matter given the data is public? \n      Yeah but ... SQL injection is covered by \n      the library functionality?\n\n\n      These are good conversations to have, but \n      not right now. In a sense, when brainstorming \n      there are 'no wrong answers'. Later on you will filter out inplausible \n      or less risky threats when you prioritise in the next part of the session. Concentrate on brainstorming \n      for now.\n      \n\n\n\n\ni. Brainstorm threats!\n\nEveryone in the group joins in with suggesting threats. Finding the widest diversity of threats is a\n       good thing, we are interested in possibilities\n      rather than the 'happy path'. A few off-the-wall ideas can help too. \n      Encourage that diversity by making sure everyone is involved and no single voice is allowed to dominate.  \n      Make sure everyone has access to pens and stickies and suggests at least one potential threat regardless of \n      background or experience.\n\nUse creative, divergent thinking and leverage the experience and various perspectives \n      present in the group. Later you will prioritise those threats which are most risky or important, so there is no danger \n      in having too many potential threats.\n\n\n\nii. Follow the data-flow lines!\n\nIf you need of inspiration, follow data-flow lines on your diagram one by one.\n      How might one of the STRIDE concepts apply, for that data-flow? Does that suggest \n      a particular threat which might need to be addressed? Working this \n      way helps identify technical mechanisms and data-flows that \n      attackers might use.\n\nThere is a data-flow to any cyber-attack, \n      just like the trusted data-flows you have drawn. Attackers use the same pathways to pivot around system that have been \n      put in place for trusted users. Cyber security losses happen when there are insufficient constraints\n      at a technical level to prevent the bad things from happening. \n      \n\n\n\niii. Capture threats on stickies\n\n\n      For each threat, spend a moment capturing it. 'SQL injection from Internet',\n      'lack of encryption in database', 'no Multi-Factor authentication' are good examples.\n      Questions are also good, such as 'do we need to store this data?', 'could there be an authorisation bypass?' \n      and 'who will revoke leavers accounts?'\n\nYou will find it is quite natural to then place these stickies \n      in a particular spot on the diagram, alongside a particular user, component or data-flow. Include just enough detail \n      so you know what the sticky means and move on to brainstorming the next threat.\n\n\n\n\n\n\nA worked example: Threats identified\n\nUsing the diagram created earlier, the team brainstormed around each \n        data-flow in the system, using STRIDE to help.\n\n\nAs we discover potential threats in the mechanisms \n        of the software, write them on stickies and annotate the lo-fi diagram.\n        \n\n\n\n\nHere's what they had come up with, at the end of the brainstorming part of the session:\n\n\n\nData-flow\n\nThreat\n\n\n\nCustomer→Identity Service\n\n\nauthentication is password based, no two-factor authentication\n\n\n\n\nCustomer→UI\n\n\na DOM based cross-site scripting (XSS) attack\n\n\n\n\nUI→Bff\n\n\nabsence/weakness of identity token validation\n\ninjection attack, such as SQL injection, stored XSS\n\nlack of logging of identity of caller\n\nbadly configured TLS transport encryption\n\nmisconfigured graphQL introspection enabled in production\n\nnetwork layer flood of traffic from a botnet\n\nfailure to prevent authenticated user from accessing someone else's details\n\nlack of regular patching could lead to remote code execution\n\n\n\n\nBff→Customer Service\n\n\nabsence or weakness of 'server to server' authentication\n\nlack of logging of identity of caller\n\noverly permissive security groups allows customer service to be accessed from Internet\n\nfailure to prevent authenticated user from accessing someone else's details\n\n\n\n\nNotice that many of the threats occurred where the data-flow crossed the authorisation boundary from \n      the Internet into the system. However threats were identified in the browser-based UI and within the backend \n      network also.\n\n\n\n\n\n\n\nPrioritise and fix\n\n\n\n      You are now going to build on the diagram the group has created, annotated with threats.\n      \n\n\n\"What are you going to do about it?\"\n\n\n      Software teams are incentivised to deliver, and rarely have unlimited bandwidth to go away and\n      address every threat identified. And some of \n      the threats may pose an insignificant risk. You need to filter down and prioritise a few most important \n      actions which you can take away and execute on effectively.\n\n\nPrioritise threats by risk\n\n\n\ni. Share knowledge useful for prioritisation\n\nIt can help to get a shared understanding of what we know about the system's risk profile. \n            Spend no more than 5 minutes on this. Roughly, there are two types of knowledge that is \n            helpful when prioritising threats. Product owners or security teams often \n            have excellent insights to share.\n\n\nBusiness value - what kind of losses put the organisation's objectives into jeapody? Is it\n            having the customer database stolen? Reputational impact due to lost business?\n\nBroader threat - what are the likely root causes of losses due to cyber security issues for this organisation?\n            Are we worried about fraud? Malicious insiders? Particularly capable hackers?\n\n\nIf nobody in the group has any insight into broader risk, then that is OK too. Access to this knowledge is not a pre-requisite. \n            You can prioritise based on technical risk alone and still get a big benefit from threat modelling. \n            \n\n\n\nii. Everyone vote for top riskiest threats!\n\n\n            This should be familiar to anyone who has done 'dot voting' in a retrospective or workshop before. \n            Everyone gets some votes and casts them for the riskiest threats. Perhaps start with three votes each \n            for your first session. But it can depend on how many people are in the group and how many threats \n            you found. The goal is to whittle down to the most valuable threats to start with. \n            Remember that risk is not just about how probable the threat is, but also the scale of\n            potential loss.\n            \n\n\n\n            Everyone casts their votes using dots, according to their own perception of the risk.\n            \n\n\n\n\n\n            Everyone needs to cast their votes by marking dots on the sticky notes.\n            Everyone gets a fixed number of votes. It is fine to vote on the same\n            threat more than once if you think that is right.\n\n\n            Voting in this way will yield good risk decisions for low investment, reflecting the diverse perspectives in the group. \n            People have an intuitive sense of risk and will be able to cast their votes with minimal prompting.\n            \n\n\n\niii. Identify the top riskiest threats\n\n\n            You will need to count up the number of votes for each threat, and mark those with the highest votes somehow.\n            Perhaps circle the riskiest threats with a pen.\n\nOften I am asked how many threats we should seek to identify in a session. The first time, three can\n            be a good number. That provides a good balance for the amount of time invested. But use your judgement and experiment. \n            There may be one very risky item- right now it makes sense just to address that. Equally four or five\n            threats can emerge from a single session.\n\n\n\niv. Take a photo of the annotated diagram, and record threats\n\n\n            Take a photograph using a mobile phone camera at this stage to capture the output of the brainstorming \n            and prioritisation. If you are working with remote tools you can take a screenshot or an export. \n            It makes a lot of sense to upload the image to your Wiki or store them in a repository \n            somewhere.\n            \n\n\n\n\n\nAdd fixes to your backlog\n\nIt is essential the time investment by the team leads to followup work. Software teams already have a powerful\n       way to represent and sequence activity to deliver software: the backlog. It is now time to work out\n       what fixes you actually need to reduce risk. Use the team's existing processes and ways of working\n       to support this (see sidebar). \n        \n\n\nRabbit Hole: Not using the team's backlog for security\n\n\n\n\n\n\n        There is no need to create a special spreadsheet or tracking document for security work. This may sound \n        obvious, but it is a very common anti-pattern. Most agile project \n        management tools have the ability to apply a custom label or tag. Perhaps create one called 'security' and \n        use it for any story which is created or expanded as a result of threat modelling.\n\nA 'security' label is useful to monitor that security work is being prioritised, but is even more useful to \n      demonstrate to risk management or cyber security specialists the details of \n      the controls that have been implemented. This information may also smooth external review and help to \n      build confidence in external stakeholders who are relying on the team to \n      build software that is secure.\n      \n\n\n\n\nv. Capture security fixes in backlog\n\nFor each prioritised threat, define concrete next steps. In the language of security, you can call \n        these 'controls', 'mitigations', 'safeguards' or simply just security fixes. These fixes may take on\n        a variety of forms. Make these fixes concrete so that it is clear when fix is \"done\", and the risk\n        has therefore been reduced.\n        \n\n\nAcceptance Criteria are the most common type of security fix to come out of threat modelling. These will \n        be added to reflect extra scope, on an existing story. For example there might be a story to perform an action, \n        and the extra acceptance criteria might be an authorisation check. Acceptance criteria should be testable.\n\nStories might crop up to implement a particular control, or get split out of an existing story if it \n        makes sense to the business analyst and the team. For example, integrating a single page app with an identity \n        system might form an 'Authenticate User' story.\n\nTimeboxed Spikes are really useful if we are either unsure if we are actually vulnerable - perhaps \n        the backend calls are sanitised automatically? - or if we are unsure of the best solution to the issue and it's worth \n        investing some developer time to find out.\n\nDefinition of Done is the set of conditions and acceptance criteria the team must meet in order to \n        consider a feature done. If you identify that all API calls need to be authenticated, authorised and logged, then \n        you should reflect that in your definition of done. This is so that you can test for it consistently before signing stories \n        off.\n\nEpics are significant bits of security architecture which are identified as part of threat modelling. \n        Examples might be introducing an identity provider, a security events system, or configuring the network in \n        a particular way. Expect threat modelling to generate epics early on in a project.\n\n\n\n\nvi. Wrap up and close out the session\n\nIf you write the fixes down on cards or paper in the session,\n        make sure someone takes responsibility for adding them to your project tracking tool or \n        agile board. Ideally the product owner will be in the threat \n        modelling session. If not then take an action to talk whoever prioritises work through\n        the threats so that you prioritise appropriately.\n\nThe best way to make sure your threat modelling has an impact is to deliver the fixes and \n      then do threat modelling again. \n      \n\n\n\n\n\n\nA worked example: Scope in the backlog\n\nWhen they voted, the team decided that three threats \n        were the most risky- and worthy of fixes.\n\nAuthorisation bypass direct to API\n\nAlthough the user has to be logged in to see the page (is authenticated), the team realised \n        there is nothing to stop unauthenicated requests direct to the API. This would have been a\n        pretty major flaw if it had made it into production! The team had not spotted it before \n         the session.\n\nThey added the following acceptance criteria to the story so it can be tested explicitly \n        as part of story sign-off.\n\n\nGIVEN an API request from the single page app to the API\n\nWHEN there is no valid authorisation token for the current user included in the request\n\nTHEN the API request is rejected as unauthorised\n\n\nXSS or injection via user input\n\nThe user profile feature allows user input for personal details, addresses and delivery \n        preferences. These details are interpreted by various legacy backend systems which may be vulnerable \n        to SQL and XML injection attacks.\n\nThe team knew that they would be implementing a lot of features in coming iterations which accept input \n        from the user and store it in the backend. Rather than add these kinds of checks to every single story they \n        added the following to the team's definition of done. These means it can be checked for at \n        story sign-off consistently.\n\n\nAll API changes tested for sanitisation of XSS, SQL and XML injection\n\n\nDenial of service from Internet\n\nThe security specialist \n        who attended the session from the cyber risk team advised that loss of revenue due to distributed \n        denial of service by online criminals had been highlighted in their work.\n\nGiven this requirement involves integrating the software with a third-party security service, in this \n        case a content delivery network- the team wrote a specific story to capture the required work. The security \n        specialist agreed to pair with the team on implementation.\n        \n\n\nAs a cyber risk specialist\n\nI need all Internet facing UI and API requests to pass through the Content Delivery Network\n\nSo that we can mitigate loss of revenue due to denial of service by criminals\n\n\nWith the work defined and ready to be added to the backlog, the threat modelling session is complete.\n        Until next time!\n\n\n\n\n\n\n\nGrow your practise\n\n\"Did we do a good enough job?\"\n\nAt the start of this guide, I introduced the three question structure. But there are actually \n      four questions, because we always need to obtain feedback and improve.\n\n\nReflect and keep improving\n\nFeedback and continuous improvement is central to managing risk.\n      Neither the systems we build nor the threats they face are simple, as I stressed at the start \n      of this guide. And every team is different- with different skills, tools, constraints and \n      personalities. There is no single way to threat model, this guide simply provides some basics \n      to get you started. Much like test-driven development or continuous delivery, threat\n      modelling rewards investment.\n\nOne way to improve is to perform a retrospective on your threat modelling efforts, once \n      you have run a few sessions. Ask what went well and what could be improved. Is the timing right? \n      Was the scope too granular? Not granular enough? What about the location or remote tools you have used? What issues \n      cropped up after the session? How long did the scope take to deliver? By asking such questions,\n       the team will adapt and build mastery over time, doubling down \n      on what works and discarding what adds little value.\n\nOver time you can grow a practise best suited to your team or organisation. Here are just a few \n      ideas for next steps:\n\n\nExperiment with different types of diagram. For something like an OAUTH2 authentication flow, \n      something like a UML sequence diagram might be better than a simple component diagram.\n\nUse domain specific threat libraries. There are many resources that can help you brainstorm\n       threats. OWASP has some great resources around Mobile or APIs.\n       There has been lots of recent interest in using the ATT\u0026CK framework.\n\nNo practise is a silver-bullet. Threat modelling is not an efficient way to find basic coding \n       or dependency issues. Complement threat modelling using automated tools in the software delivery \n      pipeline.\n\n\n       Like everything else in software, if it isn't tested then you can't consider it done. \n       Use threat modelling to discover the failure conditions to test for.\n       \n\nRun a session on your system's risk profile. This kind of analysis is \n      often focussed on the type of data you are processing and the value of your services to others-\n      in security jargon, take a deep-dive into the business value of your 'assets'.\n\nRun a session on broader threats to your system. It is common to analyse the capabilities\n      and motivations of potential attackers based on the best available information, for example threat\n      intelligence, documentation of real attack techniques and real incidents from similar systems.\n\nJoin threat modelling communities such as the Threat Modelling channel on OWASP Slack, or follow the \n      Threat Modelling SubReddit. Follow other folks doing threat modelling on Twitter.\n\n\n\n\nIn conclusion\n\nMany 'solutions' in security seem designed to keep security out of the hands of developers. \n      That does not make them bad solutions. Automated checks in the pipeline are effective at\n      finding vulnerabilties- you should use them. Penetration tests can find issues you would not yourself. \n      Platforms with secure defaults can eliminate many common threats. However each 'solution' only\n      addresses a limited class of threat. Whereas cyber-risk is not a simple thing, it is\n      multi-facated and in constant flux. Understanding this risk is at the core of effective approaches \n      to managing it.\n\nThe killer application of Threat Modelling is promoting security understanding across\n       the whole team. This is the first step to making security everyone's responsibility. \n       Just like business outcomes, quality, integration and infrastructure -  security can be central\n       to how teams think about software delivery. Rather than something bolted on at the end. \n       In my experience, a team with the muscle memory of threat modelling is a team that proactively \n       manages cyber-risk. \n      \n\nI remember being told that Threat Modelling was just too hard for most development teams. \n      I just wasn't willing to accept that was the case. I understood the chance to improve \n      how developers address security. Since then I have helped loads of teams \n      get started with Threat Modelling and their journey to understand security properly. I \n      have never come across a team who found threat modelling or security too hard.\n      Particularly when explained in an accessible way. That is exactly what I have tried to do in this guide.\n\nI believe threat modelling is a transformative practise for software development teams. A\n      collaborative and risk-based practise which can be applied continuously. \n      If you are part of a software team, please do send this article to your team and suggest a threat modelling \n      session. Or forward it to whoever is responsible for software security at your organisation. \n      By setting aside a little time to run a session you can get started. By starting to understand \n      the threats, risk and security fixes needed for your system, you are taking a step closer to effective\n      cyber security.\n\nI hope this guide helps your team to start threat modelling. At the very least a session should provide \n      value straight away- with good security stories and acceptance criteria added to your backlog. \n      This method has helped many development teams at Thoughtworks' clients\n      adopt an holistic approach to cyber security. I hope it is useful to you also.\n\n\n\n\n","author":"Martin Fowler"},{"id":"b59eacd3-13d4-4312-9b50-75b55a030dac","title":"A Proof-of-Concept of BigQuery","link":{"Scheme":"https","Opaque":"","User":null,"Host":"martinfowler.com","Path":"/articles/bigQueryPOC.html","RawPath":"","OmitHost":false,"ForceQuery":false,"RawQuery":"","Fragment":"","RawFragment":""},"text":"\nGoogle recently upgraded its enterprise offerings with an\n    improved version of Google App Engine and some new tooling. Google\n    BigQuery is the new online service for running interactive queries\n    over vast amounts of data—up to billions of rows—with great speed.\n    It’s good for analyzing large quantities of data quickly, but not\n    for modifying it. In data analysis terms, BigQuery is an OLAP\n    (online analytical processing) system, aimed at helping\n    organisations work with Big Data.\n\nWe were very interested in putting this technology to the test,\n    so we searched for a partner with a data set worthy of the label\n    \"Big\". One of our clients in the UK, AutoTrader, had both Big Data\n    and a business issue that they hoped to see if BigQuery could help\n    them with. \n\nAutoTrader is the top-ranked online site to buy and sell new\n    and used cars in the UK, and also publishes a weekly classified ad\n    magazine. Two questions a dealer wants to know the answer to when\n    a used car comes onto the lot are: how much to offer for it; and\n    how long it might take to sell it. The answers lie within\n    AutoTrader’s historical data.\n\nWe borrowed data from AutoTrader covering all ads placed on\n    their website over the last five years, including how long the ad\n    was displayed and the price of the vehicle when it was removed\n    from the site (the assumption being that was the selling price of\n    the vehicle). The test data set contained over 750 million rows,\n    and had been uploaded to Amazon’s EC2. \n\n\nGetting the data into BigQuery\n\nDealing with Big Data, file size limits are an issue. We used\n      the Unix command line 'split' to break the data file into chunks\n      of the right size, taking care to break files apart on line\n      boundaries rather than the middle of a record. Both Google Cloud\n      Storage (GCS) limits and the BigQuery import limits have to be\n      considered when doing this. \n\nAs soon as we had Google’s GCS command line tool gsutil\n      successfully installed, we used it to move the data from Amazon\n      into GCS, which turned out to be the slowest part of the whole\n      process. The total rows took around 12 hours to transfer to GCS.\n      We tried using the parallel upload facility in gsutil but this\n      just led to our EC2 instance becoming I/O bound. With hindsight,\n      we could have spent more time experimenting with Amazon regions,\n      file sizes and parallel uploads to make this process quicker.\n      \n\nWith the data in GCS, we next created a very simple text file\n      to represent the schema and used it with the Big Query command\n      line tool (bq) to set up tables in BigQuery. We checked it out\n      first with a small subset of the data, doing a few queries from\n      the BigQuery web console to be sure everything was suitable\n      before we loaded the whole dataset.\n\nThe next stage was to complete the transfer from GCS into the\n      newly defined BigQuery database. Because of the problems we ran\n      into with parallel uploads into GCS from our source on Amazon,\n      we didn’t try to use the parallel facility for the load into\n      BigQuery. Later we found we could have done this and probably\n      cut our 8 hour transfer time down to around 15 minutes.\n\nOnce we had all the data uploaded we tried a few queries. At\n      the start, these were very slow, perhaps to do with some\n      internal distributions not having happened inside of BigQuery.\n      Initially these queries were taking a few minutes, but the next\n      morning things took around 7 to 10 seconds and this remained\n      reasonably consistent for the remainder of the exercise.\n\n\n\nCreating Big Data web analytics using BigQuery and Google Charts\n\nWe decided to use Google App Engine to create a simple web\n      front-end to our queries, with Google Charts embedded for\n      interactive visualisation of the output.\n\nOur web pages made RESTful calls to our App which in turn\n      used the Java Big Query API (Python is an option as well) to\n      make RESTful calls to invoke queries on the data. The results\n      were then rendered client-side using the Google Charts\n      libraries. In addition to the interactive charts, we were able\n      to add a simple export mechanism, taking advantage of the fact\n      that BigQuery results are saved into a temporary table which can\n      be accessed via a \"job ID\". This makes for a very simple and\n      quick way to export the results of queries into GCS—and hence\n      into other Google tools or as a CSV file. All of this was\n      secured via OAUTH giving us fine-grained control over who could\n      see, access and invoke queries using the App.\n\nWe began looking into changing our web app to use\n      asynchronous mechanisms for invoking BigQuery, but ran out of\n      time. That said, it looked pretty straightforward, using the\n      jobID to query for intermediate results. Given the chance again,\n      we would go with an asynchronous approach from start.\n\n\n\nConclusions and benefits\n\nOverall we were impressed with what we were able to achieve\n      in a short amount of time with BigQuery, the APIs and utilities.\n      We were able to experiment with different ways of visualising\n      and querying a very large volume of data with a relatively low\n      investment in terms of time and without needing expensive\n      infrastructure.\n\nFor organisations longing to gain insights into very large\n      datasets, BigQuery could be a viable option that requires no\n      purchases of specialised software or additional infrastructure.\n      Even for organisations that already have an enterprise data\n      warehouse, BigQuery is an option for testing out theories, or\n      for other instances where the keepers of the data warehouse\n      place restrictions on use.\n\n\n\nIssues and Lessons\n\nWe encountered issues during the proof of concept that others\n      can learn from. \n\n\nInstalling the Google utilities\n\nMake sure the right version of Python is installed, and\n        multiple versions are not on the path, to avoid issues. We\n        found it best to use the downloadable versions of the tools,\n        as our early attempt, trying to use easy_install with an older\n        version of python caused a lot of problems. \n\n\n\nGoogle codebase examples \n\nIt proved actually quite difficult to find some working\n        Java examples of BigQuery used from within App Engine,\n        especially around the OAUTH mechanisms. But once we'd created\n        a few classes to handle the work, we had no further issues,\n        even with a four-stage redirect (as Thoughtworks uses its own\n        corporate OAUTH mechanism with Google apps). Some more thought\n        would be needed to make these mechanisms work with something\n        like webdriver and automated acceptance tests.\n\n\n\nCharts and BigQuery have slightly different JSON\n        formats\n\nAnother issue is that the JSON format returned from\n        BigQuery is slightly different from the one Charts expects.\n        While there are good reasons for this, having Charts able to\n        directly consume some of the JSON would have cut the amount of\n        code we had to create.\n\n\n\nTimestamps were a problem\n\nOur data included timestamps in a non-standard format (from\n        the Google tool’s perspective). This made it difficult to\n        construct queries that selected by time periods. While we were\n        able to work around this, it did limit some of what we'd hoped\n        to do with the data. We've passed this along to the BigQuery\n        team and expect they'll have some more flexibility around\n        date/time formats in the future.\n\n\n\nUse the web console\n\nWhile experimenting we found the BigQuery web console\n        invaluable for trying out queries before adding them into the\n        code.\n\n\n\nExperiment with gsutil and bq parallel loading\n\nWhile we encountered I/O issues loading from Amazon to GSC, we\n        later found we could have saved many hours by using the parallel load\n        from GCS into BigQuery itself. It is worth testing the various options\n        available with gsutil and bq to see which give the best performance\n        for your environment.\n\n\n\nWatch your data usage as you execute queries and develop\n        with small datasets\n\nIf you are doing a lot of testing of a full dataset, you can get\n        into several or more terabytes of data usage, which can generate a\n        high charge. Queries come back so fast, you forget that return sets\n        can be many gigabytes—and they add up fast.\n\n\n\n\n","author":"Martin Fowler"},{"id":"3eb6ecd2-8e78-442a-99e6-efb00fbe9c2f","title":"Abundant Mutation","link":{"Scheme":"https","Opaque":"","User":null,"Host":"martinfowler.com","Path":"/bliki/AbundantMutation.html","RawPath":"","OmitHost":false,"ForceQuery":false,"RawQuery":"","Fragment":"","RawFragment":""},"text":"\nAny reader of my writings will know that I'm a big proponent of\nevolutionary\n\ndesign. Despite my enthusiasm for this approach, no technique is\nperfect and I'm just as happy to report its problems as I am its\nsuccesses.\n\nI've come across this problem, although in somewhat different\nmanifestations, in two projects so far.\n\nEvolutionary Design expects the team to modify the design as the\nproject proceeds; both to cope with requirements changes and to take\nadvantage of learning. You can think of this as adding mutations to\nthe design that react to these changes. This mutation is a good and\nnecessary thing, but like many good things you can get too much of\nit.\n\nThe first project was a large project, with around a hundred\ndevelopers. In this case the over-mutation occurred because different\nsub-teams would tackle a common problem in a different way. This might\nbe by building different frameworks or by incorporating different\nexternal frameworks. \n\nThe second project was a moderate project of a dozen developers,\nbut with a significant amount of rotation. As newcomers came in they\nwould look at a previous way of tackling a problem, not understand it\nor find it deficient and go in a new direction. The trouble is things\nwouldn't complete before new people came in who found this half done\nsolution had deficiencies... you get the picture.\n\nIn both cases the net result was multiple ways of trying to solve\nthe same problem. Not just was the duplication of effort wasteful, it\nalso made the software more complex than it needed to be.\n\nI should stress that the overall design health was still pretty\ngood, compared to other systems in the same organizations. In\nparticular, the attention to automated testing allowed evolution to be\nmuch safer than the norm and both projects had significantly low\ndefect rates.\n\nAt the risk of abusing MetaphoricQuestioning, you\nmight think of this as a case where the environment hadn't killed off\nthe weaker mutations. Ideally when a competing design appears it\neither dies quickly or kills off the existing design. The problem\nhere is that neither happened. So the question becomes: how can you\nforce inferior designs out of the system?\n\nIn both cases those I spoke to felt that there was  a lack of\noverall design leadership. In the large project this was added through\nan architecture team that forged a base approach to these problems and\nthen kept a close communication about what was being done. The second\nteam hasn't tackled this issue so far, but it's seen as a need for\nsome more stable design leadership. So rather than an evolutionary metaphor, you might think of it\n\tmore like a breeder encouraging good traits and discouraging poor\n\tones. (This was an inspiration for Darwin.)\n\nMetaphors aside, I think it fundamentally comes down to following the principle \"Continuous attention\nto technical excellence and good design enhances agility.\"\nEvolutionary design requires attention, skill, and leadership. It's\njust a different\n\nsort of leadership than many commonly think.\n","author":"Martin Fowler"},{"id":"2f4a3ce7-00eb-435d-a497-ec8ed6cdd212","title":"Academic Rotation","link":{"Scheme":"https","Opaque":"","User":null,"Host":"martinfowler.com","Path":"/bliki/AcademicRotation.html","RawPath":"","OmitHost":false,"ForceQuery":false,"RawQuery":"","Fragment":"","RawFragment":""},"text":"\nA while ago I was chatting with a post-doc on his way to an\n  academic career. He was asking me about research topics wanting my\n  input as he felt I could inform him on what would be research of\n  practical use. I wasn't very helpful, but I did mention that the\n  best way to do this would be to spend some time in industry to get a\n  feel of how software development works in the wild and what problems\n  could do with some research effort. His answer to this thought was\n  very troubling. \n\nHe said he'd be up\n  to do that, but if he spent time in industry that would ruin his\n  chances of getting a job in academia. Competition for academic jobs\n  is high, and what they look it is your publication history. A year\n  or two in industry would create a gap in your publication history\n  that would be lethal to your job prospects.\n\nThe divide between academia and industry has always been an\n  awkward one for software (as indeed for other professions). My\n  contacts with academia have been stilted at best. The academics I\n  respect are, I'm told, not highly regarded within academia because\n  the things that I count as useful are usually dismissed by the\n  academic community.\n\nA good example of where this came to a head is the patterns\n  community. Those involved in the patterns world were keen to look\n  at practice to discover, package, and document techniques that had\n  been proved through experience. But this is in direct opposition to\n  academic standards which consider value to lie in novel things. My\n  work, for example, is generally dismissed because all I do is write\n  about stuff that is old hat (at least to some).\n\nI think this is a terrible shame, not because I'm looking for an\n  academic post, but because I think there is huge value in mining\n  effective techniques from the experience of software development. To\n  me it seems that trying to draw lessons from our experience is a very\n  worthwhile academic activity. By devaluing it the academic world is\n  ignoring a fruitful avenue to improve the capabilities of our profession.\n\nIf my opinion counted, I'd argue that any academic department\n  worthy of note should include a group of faculty with a long\n  experience of the day-to-day of industrial software\n  development. They would be valued on how they had reflected on this\n  experience and drew from the lessons to inform their teaching and\n  research. I'd like to see a regular rotation of people from the\n  academic to the industrial world, where it's common to see people\n  spend several years in industry, then academia, then industry again,\n  and so on.\n\nThis problem isn't only in software. A friend of mine had the\n  chief engineer role in one of the most challenging engineering\n  projects in the world. He fancied a stint in academia, but was only\n  able to get a second-class position reserved for people who weren't\n  considered to be real academics, certainly not something that was\n  tenured or would lead to tenure. I find it hard to believe that\n  students wouldn't gain an enormous amount from being taught by\n  people with a long and thoughtful experience in the profession they\n  are entering.\n\nIt's always frustrating to see communication gaps between\n  different groups within the same profession. I've become a big fan\n  using Rotation to help open up communication channels, as\n  people are the key to good knowledge transfer. Being tolerant of academic\n  rotation, indeed encouraging it, could do a great deal to make\n  academia more aware of where industry needs help and industry more\n  aware of where academics can improve practice.\n","author":"Martin Fowler"},{"id":"2b822d7f-ab3c-4016-824d-1906b3cd06c4","title":"Aggregate Oriented Database","link":{"Scheme":"https","Opaque":"","User":null,"Host":"martinfowler.com","Path":"/bliki/AggregateOrientedDatabase.html","RawPath":"","OmitHost":false,"ForceQuery":false,"RawQuery":"","Fragment":"","RawFragment":""},"text":"\nOne of the first topics to spring to mind as we worked on\n  Nosql Distilled was that NoSQL databases use different\n  data models than the relational model. Most sources I've looked at\n  mention at least four groups of data model: key-value, document,\n  column-family, and graph. Looking at\n  this list, there's a big similarity between the first three - all\n  have a fundamental unit of storage which is a rich structure of\n  closely related data: for key-value stores it's the value, for\n  document stores it's the document, and for column-family stores it's the\n  column family. In DDD terms, this group of data is an DDD_Aggregate.\n\nThe rise of NoSQL databases has been driven primarily by the\n desire to store data effectively on large clusters - such as the\n setups used by Google and Amazon. Relational databases were not\n designed with clusters in mind, which is why people have cast around\n for an alternative. Storing aggregates as fundamental units makes a\n lot of sense for running on a cluster. Aggregates make natural units\n for distribution strategies such as sharding, since you have a large\n clump of data that you expect to be accessed together.\n\nAn aggregate also makes a lot of sense to an application\n programmer. If you're capturing a screenful of information and\n storing it in a relational database, you have to decompose that\n information into rows before storing it away. \n\n\n\n\n\n\n\nAn aggregate makes for a much simpler mapping - which is why many\n  early adopters of NoSQL databases report that it's an easier\n  programming model.\n\nThis synergy between the programming model and the distribution\n model is very valuable. It allows the database to use its knowledge\n of how the application programmer clusters the data to help\n performance across the cluster.\n\nThere is a significant downside - the whole approach works really\n well when data access is aligned with the aggregates, but what if you\n want to look at the data in a different way? Order entry naturally\n stores orders as aggregates, but analyzing product sales cuts across\n the aggregate structure. The advantage of not using an aggregate\n structure in the database is that it allows you to slice and dice\n your data different ways for different audiences.\n\nThis is why aggregate-oriented stores talk so much about\n map-reduce - which is a programming pattern that's well suited to\n running on clusters. Map-reduce jobs can reorganize the data\n into different groups for different readers - what many people refer\n to as materialized views. But it's more work to do this than using the\n relational model.\n\nThis is part of the argument for PolyglotPersistence -\n use aggregate-oriented databases when you are manipulating clear\n aggregates (especially if you are running on a cluster) and use\n relational databases (or a graph database) when you want to\n manipulate that data in different ways.\n","author":"Martin Fowler"},{"id":"b6b49338-b36d-49ba-8dd6-af177f1f5b61","title":"Activity Oriented","link":{"Scheme":"https","Opaque":"","User":null,"Host":"martinfowler.com","Path":"/bliki/ActivityOriented.html","RawPath":"","OmitHost":false,"ForceQuery":false,"RawQuery":"","Fragment":"","RawFragment":""},"text":"\nAny significant software development effort requires several different activities to\n  occur: analysis, user experience design, development, testing, etc. Activity-oriented\n  teams organize around these activities, so that you have dedicated teams for\n  user-experience design, development, testing etc. Activity-orientation promises many\n  benefits, but software development is usually better done with\n  OutcomeOriented teams.\n\n\n\n\n\n\n\nTraditionally, big businesses with large IT departments (Enterprise IT) have tended\n  to execute IT development projects with a bunch of activity-oriented teams drawn from a\n  matrix IT organization (functional organization). The solid-lined arms of the \n  matrix (headed by a VP of development, testing and so on) are usually along activity \n  boundaries and they loan out “resources” to dotted-lined project or program organizations. \n  Common justifications for doing so include:\n\n\nIt helps standardization of conventions and techniques in development if all\n    developers report into a single organization (arm of the matrix). Same for testing\n    etc.\n\nIt helps the cause of mentoring, training and nurturing the competency in general\n    if all developers have long-lived development/engineering managers. Same for testing\n    etc.\n\nIt helps maximize utilization of talent (and thereby improve cost-efficiency) by\n    staffing projects from pools of supposedly fungible developers, testers etc.\n\n\nHowever, activity-oriented teams are prone to optimize for their own activity and not\n  for the bigger picture of delivering useful software. This is a consequence of what they\n  are held responsible for and how they are measured. It is common for a team of only\n  developers to only be measured by their velocity. If they are only tasked with\n  delivering scope, they will not think about whether it is going to solve the problems it\n  was meant to. Even if they do so, they may be discouraged by the product management team\n  - another activity-oriented team that is only responsible determining the spec.\n\nOrganizing by activity gets in the way of lowering batch size of work that is\n  handed-off between teams. A separate team of testers won’t accept one story at a time\n  from a team of developers. They’d rather test a release worth of stories or at least all\n  stories in a feature at a time. This lengthens the feedback loop, increases end-to-end\n  cycle time and hurts overall responsiveness.\n\nHigh speed IT calls for motivated teams. Autonomy is a key motivator for teams.\n  However, activity oriented organization can only be granted so much autonomy because\n  they tend to use it for optimizing their sphere of activity.\n\nA variation of activity-oriented organization are super-specialized teams that may result\n  in the following ways:\n\n\nTool or skill centric teams: e.g. a WebSphere Portal Server team or a BizTalk\n    team\n\nArchitectural Layer teams: e.g. a presentation layer team, middleware team, data\n    layer team.\n\n\nThey are problematic because they have a narrow focus and they tend to optimize for\n  team performance rather than the big picture. For sure, some tools may need specialists\n  but that is no reason to isolate them in a separate team. Specialization isn’t the\n  problem; organizing along lines of specialization is.\n\n\nWhat works better\n\nSoftware development is an iterative design process. In order to achieve true\n    iteration and realize the value of fast feedback, its activities need to be performed\n    with a single team (having a common reporting line) as far as possible. Many internet\n    business and independent software vendors (ISVs) already operate in this way.\n\n\nSriram's book goes into more detail on how to build an effective IT organization\n      using agile approaches.\n\n\nActivity-oriented organization may be attractive from the perspective of maximizing\n    utilization but it comes in the way of maximizing value-addition and end-to-end\n    responsiveness. In today’s business climate, market responsiveness (time-efficiency)\n    is more important than cost-efficiency especially when it comes to capital expenditure\n    items such as software development. Besides, solid-line reporting should be aligned to\n    the most important goals which are responsiveness and value-addition.\n\nIn order to effectively nurture competencies in the absence of an aligned reporting\n  structure, set up communities of practice along with expert community leads who have the\n  ability to mentor and the budget to organize community events, buy books and arrange for\n  training. These communities also help with standardizing conventions and techniques\n  without going overboard with standardization. As for having a stable reporting manager,\n  it is only an issue in project-centric IT. A BusinessCapabilityCentric setup allows\n  for stable reporting managers.\n\n\n\nAcknowledgements\n\n    Thanks to Bill Codding, David Wang (Wang Wei), James Lewis, Kief Morris, Patrick Kua, Patrick Sarnacke, Steven Lowe and Venkatesh Ponniah for their comments. Special thanks to Martin Fowler for his guidance with the content, help with publishing and for the nice illustrations.\n  \n","author":"Martin Fowler"},{"id":"c839968d-d6f0-4605-be99-70ace13e3082","title":"A Cherry Picker's Guide to Doctor Who","link":{"Scheme":"https","Opaque":"","User":null,"Host":"martinfowler.com","Path":"/articles/doctor-who.html","RawPath":"","OmitHost":false,"ForceQuery":false,"RawQuery":"","Fragment":"","RawFragment":""},"text":"\nDoctor Who is a British TV series with a long history. Its\n    first broadcast was in 1963, and its current incarnation has run\n    for eight seasons. Most TV series require you to start at the\n    beginning and watch every episode, but although that has its\n    delights, you don't have to do that with Doctor Who, since many of\n    its best episodes are written so that you can enjoy them as a\n    self-contained film. This is my personal suggestions of how to\n    watch Doctor Who by cherry picking individual episodes.\n\nFor most episodes of Doctor Who, all you need to know is the\n    rough premise. The Doctor is a Time Lord, a human looking alien,\n    who travels through time and space with various companions\n    (usually human, young, and female). This setup allows writers to\n    set self-contained stories in any setting: historical, futuristic,\n    and current. His spaceship is called the TARDIS, from the outside\n    it looks like a blue British Police Box from the 1960s, its inside\n    is a large trans-dimensional space. Each episode usually sees the\n    Tardis appear and the Doctor and his companion find some sticky\n    predicament focused on some malevolent alien. They resolve the\n    situation, often with a notable lack of violence. The Doctor is\n    strikingly not like the usual action hero, he rarely uses weapons:\n    relying on his wits to defeat any threats.\n\nDoctor Who is one of the longest running TV series, it began in\n    November 1963. You can divide its history so far into two broad\n    periods: Classic Who runs from its beginning until 1989 when\n    the series was cancelled. New Who runs from its reboot in 2005 until\n    today, where it's still going strong. Classic Who was a children's\n    show that appealed to many adults. The writers of New Who mostly\n    fell in love with Classic Who as children (as I did) and went on\n    to write a show that should appeal equally to adults and children.\n    I saw my first episode of New Who with two friends my own age and\n    their pre-teen daughters, who hadn't seen Classic Who as we elders\n    had.\n\nIf you want to explore Doctor Who from scratch, I would start\n    with New Who. Classic Who has its charms, and some remarkably good\n    serials. But the quality of Classic Who varies from very good to\n    truly awful, and comes with special effects that vary from cheesy to\n    truly awful. So although I'll suggest a couple of Classic Who serials later\n    on, most new viewers should start at New Who. Although there is some\n    references to Classic Who, you can appreciate New Who without\n    knowing anything about Classic Who.\n\nI'm focusing on cherry picking here, but plenty of people enjoy\n    starting from the beginning of New Who and taking a\n    completist approach through every episode. There are certainly rewards for the\n    completist approach, with some good internal references and many great\n    episodes that you can't appreciate by cherry picking. You don't\n    need any advice to be a completist, but I will say that you should\n    give it until at least the sixth episode (Dalek)\n    before you decide it's not for you. In particular the fourth and\n    fifth episodes (Aliens of\n    London / World War Three) are two of the weakest\n    episodes in New Who, with some annoyingly juvenile humor.\n\nFor my picks I'm selecting episodes which are both my favorite\n    episodes, but also episodes that don't rely on any surronding\n    story arc. If it is valuable to see other picked episodes first,\n    I'll mention that. There's also an argument for watching some\n    individual series completely, and I'll mention which of those I\n    think are worth considering for that.\n\nThe first question is where does a cherry picker begin, and\n    I generally advise starting with Blink, which is on most\n    people's short list for greatest Who episode ever. It actually\n    doesn't feature the Doctor that much, and mostly ignores the\n    companion of that series. But it's a clever plot, superbly acted\n    by a young Carey Mulligan, and lots of wit in the script.\n    Deservedly it won a BAFTA drama award, a rare event for a sci-fi\n    story. [1] The episode was written by Stevan\n    Moffat, who since went on to be the showrunner for Doctor Who.\n    He's also the co-showrunner for Sherlock,\n    and the writer of Jekyll (a\n    superb six-episode miniseries).\n     I rate him with Joss Whedon as one of the best writers of our\n    time.\n\nSo start with Blink, but after that you can mostly\n    pick and mix as you like. I'm going to list my picks\n    chronologically series by series, but you don't have to do them in\n    that order. If any episodes require you to see some others first,\n    I'll point that out. You should get used to different actors\n    playing the Doctor - there is a clever techno-babble reason why\n    multiple actors can play the same character, which doesn't affect\n    a cherry picking watcher. Each actor emphasizes different aspects\n    of the same character. Companions are different people, but again\n    the cherry picking choices I've made don't rely too much on their\n    ongoing story.\n\nThe first series was the reboot of Doctor Who, making it return\n    to the screen after a silence of fifteen years. The driving force\n    for the reboot was Russel T Davies, who already had garnered a\n    fine reputation as a TV writer. Playing the Doctor in this series\n    is Christopher Eccleston, sadly in his only series as the Doctor,\n    with Rose (Billie Piper) as his companion. The standout episode of\n    the first series is Moffat's The Empty Child / The Doctor\n    Dances. Most movies\n    aren't this good, and this double episode introduced the Moffat\n    approach of combining fright and wit. The other cherry pick I'd\n    make from the first series is Dalek, which reintroduces the\n    Doctor's iconic enemy in a story for them that still hasn't been\n    surpassed.\n\nThe first series also has one of the better story arcs, so may\n    be worth doing the full series, if only to really enjoy the final\n    two-parter which isn't worth watching without that context. (A tip\n    if you do watch the whole series: don't watch the trailer for the\n    next episode at the\n    end of Boom Town, as\n    it gives away an important part of the plot of Bad Wolf.)\n\n\nTorchwood\n\nThe series arc of the second series introduces Torchwood, an\n      UK based alien monitoring agency. The arc is pretty weak\n      compared to series 1, but Torchwood ended up\n      spun off into its own series, aimed at an adult\n      audience, with Captain Jack returning to headline. While he got\n      it going Russel Davies didn't put much energy into it. In my\n      view the first two series of Torchwood are best forgotten, the\n      best episodes could barely beat the weakest episodes of Doctor\n      Who. \n\nFor the third series, however, Davies wrote the whole thing\n      himself as a 5 part serial, Children\n      of Earth, shown over consecutive nights. It's in a whole\n      different quality class, very engrossing with a particularly\n      good performance from Peter Capaldi (who later went on to play\n      The Doctor). Beware though that this series makes King Lear and\n      Hamlet seem like feel-good, Hollywood, happy-ending stories.\n\nDavies went on to make a fourth series: Miracle Day. I\n      haven't seen it (and wasn't tempted by what I heard from those\n      that did.)\n\n\nThe second\n    series has David Tennant playing the tenth Doctor [2]. My main cherry pick here\n    is again the Moffat episode:The Girl in the Fireplace. I also think\n    the two parter The Impossible Planet / The Satan\n    Pit is worth\n    watching.\n\nThe third series continued with David Tenant but brought in a new\n    companion: Martha. Blink comes from this series, but another\n    outstanding highlight of series three is Human Nature / The Family\n    of Blood. You do need to have watched a few Whos to really get into\n    the Doctor's nature and character to appreciate this one, (I'd\n    suggest watching the series 4 picks first). It\n    was the first episode for me to reach the same heights as The Empty Child / The Doctor\n    Dances\n\nFor the fourth series Tenant was joined by the already well-known\n    comedienne Catharine Tate as the companion Donna. Again Moffat came\n    up with an outstanding thriller Silence in the Library /\n    Forest of the Dead. But there are also other great highlights here.\n    The Unicorn and the Wasp is probably the most out-and-out comedy in\n    Who, a wonderful send-up of the Agatha Christie country house murder\n    mystery (featuring Agatha herself, as only Who can). Russel Davis also\n    came up with a taut character-driven thriller Midnight. I also\n    really enjoyed Turn Left, but am not sure whether to recommend it\n    here as it makes many references to non-cherry-picked episodes.\n    However I think you can still enjoy it without  following\n    those references, if only for some great acting from Tate and\n    Bernard Cribbins. \n\nAfter the fourth season there was a year of specials, which\n    generally isn't counted as a series. From this set, I'd pick The Waters of Mars, which was a fine take on the \"base under seige\"\n    style of Who plot.\n\nWith series 5 there was a wholesale change. Russel Davies gave up\n    the role of showrunner, handing over to Moffat. Tenant also gave up\n    the role of the Doctor. Matt Smith became the eleventh Doctor, and\n    we got a new companion in Amy, later joined by her husband Rory. The\n    first two episodes of the Moffat era: The\n    Eleventh Hour and The Beast Below are both\n    worth picking. I also would pick the two parter The Time of Angels /\n    Flesh and Stone,\n    although for this one it's important to have seen Blink first (to\n    know about The Weeping Angels) and Silence in the Library /\n    Forest of the Dead to know about River Song. River Song becomes an\n    important character in various episodes in series 5 and 6 after\n    this. Series 5 is also another good series to watch all the way\n    through, with good development up to the finale. But if you want to\n    stay with cherry picking, do watch Amy's Choice, and perhaps Vincent and the Doctor[3].\n\nOne of the traditions of New Who is to have a Christmas special\n    episode for broadcast on Christmas Day. Davis did the earlier\n    christmas specials, and I don't put any of them on my pick list. But\n    I do pick Moffat's first special A\n    Christmas Carol, as a clever remix of the Scrooge\n    story starring Michael Gambon.\n\nWith series 6, Moffat decided to break the pattern of earlier\n    series and start the series with a\n    two-parter The\n    Impossible Astronaut / Day of the Moon\", which opens\n    the series with a bang. The unresolved question from this episode\n    may entice you to watch this whole series, and that's not a bad choice.\n    If you would rather cherry-pick, you shouldn't miss The Doctor's Wife (written by Neil\n    Gaiman) and The Girl Who Waited.\n\nSeries 7 is a series of two distinct parts, with a halfway\n    split seeing the departure of Amy and Rory and the arrival of the new companion\n    Clara. My cherry-picking suggestion here is to see The Asylum of the Daleks first (which sort-of introduces Clara). Then see\n    The Snowmen which picks up the Doctor after he is sadly\n    separated from Amy and Rory and sort-of introduces him to Clara.\n    Moffat does some clever meta-textual stuff here, which you can\n    only appreciate fully if you're a completist but those two\n    episodes still stand strong even without fully getting the\n    meta-text in the background. I would also pick out The Crimson Horror, which is a comedy with a fun performance from\n    Diana Rigg.\n\nSeries 7 finishes with the Big Event episode of Who so far, the\n    50th anniversary special (The Day of the Doctor), broadcast 50 years after the first\n    episode. Naturally there's a huge amount of references for the fans\n    in this episode, which unites both Tenant and Smith's Doctor,\n    together with John Hurt's sort-of Doctor. It still stands alone for\n    pickers, so don't let the lack of background stop you watching it.\n    Before you do, however, catch the minisode: the The Night of the Doctor\n    on youtube. It's a remarkable coup of story-writing to get so much\n    into a seven minute episode (although it does help to know that Paul\n    McGann played the eighth Doctor, the one that immediately preceded the New Who\n    period).\n\nWith series 8, Peter Capaldi takes on the role of the twelfth Doctor,\n    and brings a darker, less charming Doctor to the scene. Not everyone\n    liked this take on the character, but I do as it reminds me of My\n    Doctor (the 3rd - Jon Pertwee). It also is my favorite full-series\n    arc, with some great character development and interplay between the Doctor and\n    Clara. For cherry pickers, however, I'd pick out \n    Into the Dalek, Listen, Kill the Moon, and Mummy on the Orient Express, but ensure you see the\n    others before you see Mummy on the Orient Express since that last episode\n    gains a lot from getting familiar with Capaldi's Doctor.\n\nAs I write this, the latest Doctor Who episode was Last Christmas, the latest\n    Christmas Special, which is also a pick for its wonderful mashup of\n    Alien, Santa Claus, and another movie you'll recognize.\n\nSo how about Classic Who? There's lots of Classic Who, but if\n    you're going to explore it, I should pick out a couple of places to start.\n    Unlike New Who which goes for single or double episodes, Classic Who\n    had short serials of 4-6 half hour episodes. I would start with City of Death, which has the iconic Tom Baker as the fourth Doctor,\n    Romana as a Time Lord companion, and a script that clearly shows it\n    was part-written by Douglas Adams. After that I have to point you to\n    some 3rd Doctor (since he's My Doctor), and pick out\n    Carnival of\n    Monsters, written by Robert Holmes, generally rated as the\n    greatest of the Classic Who writers, and features Jo Grant who was\n    the companion I best remember. \n\n\n","author":"Martin Fowler"},{"id":"d281918c-a8d2-46a3-8e0c-e75720bc172e","title":"A Language Workbench in Action - MPS","link":{"Scheme":"https","Opaque":"","User":null,"Host":"martinfowler.com","Path":"/articles/mpsAgree.html","RawPath":"","OmitHost":false,"ForceQuery":false,"RawQuery":"","Fragment":"","RawFragment":""},"text":"\n(If you aren't familiar with language oriented programming and\nlanguage workbenches - or at least my usage of the terms, you should\nread my  outline article on Language\nWorkbenches. This article discusses an example of using a language\nworkbench and assumes you'll be familiar with the concepts I discussed\nin that article.)\n\nOne of these language workbenches is the Meta-Programming\nSystem (MPS) from JetBrains. As I\nwas writing my language workbench article I wanted to include a more\nsubstantial example with in an actual language workbench to help give\nyou a better picture of what working with such a tool would be like.\nThe example ended up being a big one to describe, so I decided to\nbreak it out into its own article.\n\nI decided use MPS not because of any opinions about which\nlanguage workbench is the best (after all they are still in very early\nstages of development), but simply because that the JetBrains office\nis just down the road from where I live. A short distance to drive\nmakes collaboration much easier. So as you read this, remember that\nonly part of the point is to look at MPS. The real point of the\narticle is to give you a feel of what this class of tools is like to\nuse. On the surface each tool is quite different - but they share many\nunderlying concepts.\n\nJetBrains have opened up the MPS in their Early Access Program,\n\t\twhich allows people to download a development version of MPS to\n\t\tplay with it. You'll find the example from this article in\n\t\tthere. Remember, however, that  the tool is still very much under development\n\t\t\tso what you see when you look at it now may be very different\n\t\t\tfrom what I see as I write this. In particular certain screens\n\t\t\tmay have changed and I don't expect to keep the screenshots here\n\t\t\tup to date with each change. There are also\n\t\t\tnumerous rough edges which are typical of a new kind of tool\n\t\t\tthat still being worked on. However I think it's still worth\n\t\t\tlooking at because the principles are what counts.\n\n\nAgreement DSL\n\n This example uses a\npattern that I've come across several times, which I now call\nAgreement\nDispatcher. The idea behind an agreement dispatcher is that a\nsystem receives events from the outside world and reacts to them\ndifferently due to various factors, of which a leading one is the\nagreement between the host company and the party that the event was\nabout.\n\nPerhaps the easiest way to talk about this further is to show\n\t\t\tan example of the DSL I'll be using as an example.\n\n\nFigure 1: Agreement DSL for a\n\t\t\tregular plan.\n\n\n\n\nThis piece of DSL indicates how a notional utility company\n\t\t\treacts to events for customers on its regular plan. The\n\t\t\tagreement definition consists of values and event handlers, both\n\t\t\tof which are temporal - their values change over time. \n\nThis agreement has one value - the base rate that the\n\t\t\tcustomer is charged for electricity. From 1 Oct 1999 it was set\n\t\t\tat $10 per KwH, on the 1st December it was raised precipitously\n\t\t\tfor $12/KwH.\n\nThe agreement shows reactions to three kinds of events: usage\n\t\t\t(of electricity), a service call (such as someone coming in to\n\t\t\tfix a meter), and tax. The handlers are temporal in the same way\n\t\t\tas the base rate; we can see that the handler for service calls\n\t\t\talso changed on December 1st.\n\nThe handler indicates a simple reaction - the posting of some\n\t\t\tmonetary value to an account. The account is stated directly in\n\t\t\tthe DSL, the amount is calculated using a formula. The formula\n\t\t\tcan include values defined in the agreement together with\n\t\t\tproperties on the event. Usage events include a usage property\n\t\t\tthat indicates how many KwH of electricity were used in this\n\t\t\tbilling period. The posting rule for the USAGE event indicates\n\t\t\tthan when we get a usage event we post the product of this usage\n\t\t\tand the base rate to the customer's base usage account.\n\n\nFigure 2: Another DSL fragment for\n\t\t\tlow pay cases.\n\n\n\n\n Figure 2 shows a second\n\t\t\tagreement, this one for low paid people on a special plan. The\n\t\t\tonly interesting addition to this is that usage formula here\n\t\t\tinvolves a conditional, expressed using Excel syntax.\n\nThe first thing to note about these fragments is that they\n\t\t\tare very domain oriented and readable in terms of the\n\t\t\tdomain. Although the COBOL inference hangs over me, I'd venture\n\t\t\tto say they are readable to a non-programmer domain expert.\n\nThese DSL fragments will generate code that fits into a\n\t\t\tframework written in Java, indeed these DSLs describe the same\n\t\t\tscenarios that I used in the description of agreement dispatcher. \n\nFor the sake of comparison,\nhere's the same configuration code written in Java.\n\npublic class AgreementRegistryBuilder {\n\n    public void setUp(AgreementRegistry registry) {\n        registry.register(\"lowPay\", setUpLowPay());\n        registry.register(\"regular\", setUpRegular());\n    }\n\n    public ServiceAgreement setUpLowPay() {\n        ServiceAgreement result = new ServiceAgreement();\n        result.registerValue(\"BASE_RATE\");\n        result.setValue(\"BASE_RATE\", 10.0, MfDate.PAST);\n        result.registerValue(\"CAP\");\n        result.setValue(\"CAP\", new Quantity(50, Unit.KWH), MfDate.PAST);\n        result.setValue(\"CAP\", new Quantity(60, Unit.KWH), new MfDate(1999, 12, 1));\n        result.registerValue(\"REDUCED_RATE\");\n        result.setValue(\"REDUCED_RATE\", 5.0, MfDate.PAST);\n        result.addPostingRule(EventType.USAGE,\n                new PoorCapPR(AccountType.BASE_USAGE, true),\n                new MfDate(1999, 10, 1));\n        result.addPostingRule(EventType.SERVICE_CALL,\n                new AmountFormulaPR(0, Money.dollars(10), AccountType.SERVICE, true),\n                new MfDate(1999, 10, 1));\n        result.addPostingRule(EventType.TAX,\n                new AmountFormulaPR(0.055, Money.dollars(0), AccountType.TAX, false),\n                new MfDate(1999, 10, 1));\n        return result;\n    }\n    public ServiceAgreement setUpRegular() {\n        ServiceAgreement result = new ServiceAgreement();\n        result.registerValue(\"BASE_RATE\");\n        result.setValue(\"BASE_RATE\", 10.0, MfDate.PAST);\n        result.setValue(\"BASE_RATE\", 12.0, new MfDate(1999, 12, 1));\n        result.addPostingRule(EventType.USAGE,\n                new MultiplyByRatePR(AccountType.BASE_USAGE, true),\n                new MfDate(1999, 10, 1));\n        result.addPostingRule(EventType.SERVICE_CALL,\n                new AmountFormulaPR(0.5, Money.dollars(10), AccountType.SERVICE, true),\n                new MfDate(1999, 10, 1));\n        result.addPostingRule(EventType.SERVICE_CALL,\n                new AmountFormulaPR(0.5, Money.dollars(15), AccountType.SERVICE, true),\n                new MfDate(1999, 12, 1));\n        result.addPostingRule(EventType.TAX,\n                new AmountFormulaPR(0.055, Money.dollars(0), AccountType.TAX, false),\n                new MfDate(1999, 10, 1));\n        return result;\n    }\n\n}\n\n\nThe configuration code isn't exactly the same. The posting\n\t\t\trule carries a taxable boolean marker that we haven't added to\n\t\t\tthe DSL yet. In addition the formulae are replaced by various\n\t\t\tJava classes that can be parameterized for the most common cases\n\t\t\t- this if often better than trying to dynamically create\n\t\t\t  formulae in a Java solution. But I think the basic message\n\t\t\t  comes across - it's much harder to see the domain logic in the\n\t\t\t  Java, because Java's grammar does get in the way. This is\n\t\t\t  particularly so for a non-programmer.\n\n(If you're interested in how the resulting framework\nactually works, take a look at the agreement\n\ndispatcher pattern - I'm not going to go into it here. The example\nin that pattern is similar, but not exactly the same.)\n\nYou may have noticed that the DSL examples used screen shots\n\t\t\trather than text - and that's because although the DSLs look\n\t\t\tlike text they aren't really text. Instead they are projections\n\t\t\tof the underlying abstract representation, projections that we\n\t\t\tmanipulate in the editor.\n\n\nFigure 3: Adding a new rate\n\n\n\n\n Figure 3 indicates\n\t\t\tthis. Here I'm adding new base rate. The editor indicates the\n\t\t\tfields I need to fill in, putting in appropriate values as\n\t\t\trequired. I don't actually type much text - often my main task\n\t\t\tis picking from pick lists. At the moment the date goes in as\n\t\t\tstructured figures, but in a fully developed system you could\n\t\t\tuse a calendar widget to enter the date.\n\nOne of the most interesting elements of this is the use of\n\t\t\texcel-style formulae in the plan. Here's the editor as I add a\n\t\t\tterm to a formula.\n\n\nFigure 4: Editing a\n\t\t\tformula.\n\n\n\n\nNotice that the pop up includes various expressions you might\n\t\t\twant in a formula, plus the values defined in the plan, plus the\n\t\t\tproperties on the event that's being handled in this\n\t\t\tcontext. The editor is using a lot of knowledge of the context\n\t\t\tto help the programmer enter code correctly - much as\n\t\t\tpost-IntelliJ IDEs do.\n\nAnother point about the formulae is that they come from a\nseparate language from the language used to define agreements. So any\nDSL that needs to use excel-like formulae can import formulae to their\nlanguage without having to create all the definitions for themselves.\nFurthermore these formulae can incorporate symbols from the language\nthat's using the formula language. This is a good example of the kind\nof symbolic integration that language workbenches strive for. You need\nto be able to take languages defined by others, but at the same time\nweave them as seamlessly as possible into your own languages.\n\n(As a point of full disclosure, this formula language was in\n\t\t\tfact written in response to developing this example, but it is\n\t\t\tseparated so it can be used by other languages. This is an\n\t\t\taccident of the fact we are seeing a tool in development,\n\t\t\ttogether with MPSs development philosophy: find interesting\n\t\t\tapplications of MPS\n\t\t\tand use the needs of these applications to drive the features\n\t\t\tand design of MPS. This is a development philosophy I\n\t\t\tfavor.)\n\nThat last screen-shot shows another important point. You'll\n\t\t\t\tnotice that I didn't finish working on the new rate when I\n\t\t\t\tswitched over to the formula. One of the past problems with these\n\t\t\t\tkind of intelligent, or structured, editors is that they couldn't\n\t\t\t\tdeal with incorrect input. Each bit of input needs to be\n\t\t\t\tcorrect before you move on. Such a requirement is a big\n\t\t\t\tusability problem. When programming you need to be able to\n\t\t\t\tswitch around easily - even if that means leaving invalid\n\t\t\t\tinformation in place. The consequence of this, for a\n\t\t\t\tprojecting editor, is that you need to be able to handle\n\t\t\t\tinvalid information in your abstract representation. Indeed\n\t\t\t\tyou want to be able to do this and still be able to function\n\t\t\t\tas much as possible. In this case one option would  be\n\t\t\t\t to generate code from the plan, ignoring those temporal\n\t\t\t\telements that are in error. This kind of robust behavior in\n\t\t\t\tthe face of wanton invalidity is an important feature of\n\t\t\t\tlanguage workbenches. \n\nThe example in MPS here uses a text-like projection. MPS,\nthus far, focuses on this kind of projection. In contrast the\nMicrosoft DSL tools focus on a graphical projection. I expect that as\ntools develop they will offer both textual and graphical projections.\nDespite the modeling crowd's obsession with saying \"a picture is worth\na thousand words\", textual representations are still very useful. I\nwould expect mature language workbenches to support both textual and graphical\nprojections - together with projections that many people don't think\nas programming environments\n\n\n\nDefining the Schema\n\nNow we can see what the language looks like, we can take a\n\t\t\t\tlook at how we define it. I'm not going to go through the\n\t\t\t\tentire language definition here, I'm just going to pick out a\n\t\t\t\tfew highlights to give you a feel how it works.\n\n\nFigure 5: The schema for Plan\n\n\n\n\n Figure 5 shows the\n\t\t\t\tschema for the plan construct. (I've also shown on the left\n\t\t\t\tthe list for other concepts in this agreement language.) If\n\t\t\t\tyou've done any data modeling, or in particular meta-modeling,\n\t\t\t\tthis shouldn't have any surprises. I'm not going to explain\n\t\t\t\tall elements of the definition here - only the highlights. As\n\t\t\t\tusual, remember that this is currently in flux, it probably\n\t\t\t\tdoesn't look quite like this any more.\n\nWe define a concept, allow\n\t\t\t\tit to extend (inherit from) other concepts. We can give our\n\t\t\t\tconcept properties and links (similar to attributes and\n\t\t\t\trelationships) both at the instance and concept (class\n\t\t\t\tlevel). With links we indicate the multiplicity (in both\n\t\t\t\tdirections), and the target concept.\n\nSo in this case we see that a plan is made up of multiple\n\t\t\t\tvalues and events, each of which have their own\n\t\t\t\tdefinitions.  Figure 6\n\t\t\t\tshows the definition for event, which is pretty simple.\n\t\t\t\t\n\n\nFigure 6: Schema for\n\t\t\t\tEvent\n\n\n\n\nWe get something new in the posting rule temporal property.\nBoth values and posting rules end up being governed by this kind of\ntemporal rule, so it makes sense to factor out the common ability to\nhave date-keyed logic. So we have both a temporal property definition\n( Figure 7) and extend\nthat with a temporal property for posting rules ( Figure 8).\n\n\nFigure 7: Schema for\n\t\t\t\ttemporal property\n\n\n\n\n\nFigure 8: Schema for the\n\t\t\t\ttemporal link to posting rules.\n\n\n\n\nIn this case the temporal property defines the notion of a\n\t\t\t\tvalidity date and a value. The posting rule temporal property\n\t\t\t\textends this - but does so in a way that's slightly different\n\t\t\t\tto inheritance in an object-oriented language. Rather than\n\t\t\t\tadding new link, it specializes this value link saying that it\n\t\t\t\tcan only link to posting rules. This is similar to what you\n\t\t\t\twould achieve with generics in programming language. This idea\n\t\t\t\tof specializing a relationship is present in several modeling\n\t\t\t\tlanguages (including UML). I didn't find it terribly useful\n\t\t\t\tfor most modeling but it is rather handy for\n\t\t\t\tmeta-modeling. You can think of it as a particular form of\n\t\t\t\tconstraint.\n\nFinally I'll show how the posting rule itself is defined. \n\n\nFigure 9: Schema for\n\t\t\t\tposting rule.\n\n\n\n\nIt extends a concept called formula, which is actually part\n\t\t\t\tof a separate formula language.\n\n\nFigure 10: Schema for formula\n\t\t\t\tfrom a separate language.\n\n\n\n\nSo from this you can get a sense of what's involved to set\n\t\t\t\tup a schema in MPS. For each concept you edit the definition\n\t\t\t\tmaking the various links between the elements. I suspect that\n\t\t\t\ta data model or UML like class diagram would work better here\n\t\t\t\t- this is the kind of thing that works nicely in diagrammatic\n\t\t\t\t  form. However this style of editor also works pretty well\n\t\t\t\t  and can allow you to enter a new language schema fairly rapidly.\n\nIf you're thinking what I hope you're thinking, you'll have\n\t\t\t\tnoticed something else. The screens for editing schemas look\n\t\t\t\tawfully like the screens for editing the DSL. As you may guess\n\t\t\t\tthere is a DSL for editing schemas - called the structure\n\t\t\t\tlanguage in MPS. I edit the schema using the editor that's\n\t\t\t\tpart of the that DSL. This kind of metacircular bootstrapping\n\t\t\t\tis common in language workbenches. \n\n\n\nBuilding Editors\n\nNow lets take a look at how we define editors in\n\t\t\t\tMPS.\n\n\nFigure 11: The editor definition\n\t\t\t\tfor plans.\n\n\n\n\n Figure 11 shows the editor\nfor a plan. In general we build an editor for each concept in our\nmodel. (It's not quite one for one, but that's a good place to start\nthinking.)To define the editor for plans we use the editor for editors\n(it's getting hard to avoid the metacircularity here.) Editors are\ndefined as  a hierarchy of cells. The leaves of the hierarchy can be\nconstants, or references to elements in the the schema. The editor\neditor (which we are looking at now) uses some symbols to help delimit\nparts of the editor. Although these symbols are a bit cryptic it's\nimportant not to get worried about notation with language workbenches\nsince notation is very easy to change.\n\nThe top of this cell hierarchy is a cell collection for the\n\t\t\t\twhole editor. I select this by selecting the '[/' cell. \n\n\nFigure 12: Selecting the top of\n\t\t\t\tthe cell hierarchy.\n\n\n\n\nWhen you're working with the editor editor, the inspector\n\t\t\t\tframe (bottom left) which we didn't use earlier now becomes\n\t\t\t\timportant. The inspector is used in the same way that\n\t\t\t\tproperty editors are used in GUI builders. Here the inspector\n\t\t\t\tshows that we have a vertical cell collection. The sub-cells\n\t\t\t\tare:\n\n\nThe row beginning [\u003e plan\n\nA blank row\n\nThe row including % value % and its\n\t\t\t\t\tfollowing row.\n\nAnother blank row\n\nThe row including % event % and its following row.\n\n\nAs you can see, one of the problems with this projection is\n\t\t\t\tthat it's hard to figure out the actual cell hierarchy. It's\n\t\t\t\talso questionable to use blank cells to show\n\t\t\t\tindentation and whitespace. I expect there'll be a good bit\n\t\t\t\tmore work on how to make editor editors more usable in the future.\n\nThe three non-blank rows correspond the line naming the\n\t\t\t\tplan, the lines of values, and the lines of events in the plan\n\t\t\t\teditor.\n\n\nFigure 1: Here's the example\n\t\t\t\tfor regular plans again. See how the three non-blank lines in\n\t\t\t\tthe plan editor correspond to three content areas in the plan:\n\t\t\t\tname, values, and events.\n\n\n\n\nNow I'll dig into the first of these content areas, the\n\t\t\t\tname of the plan. It helps that this is the simplest area to\n\t\t\t\tdig into, but even so it's hard to describe it in an article\n\t\t\t\tlike this because the editor editor uses the inspector to\n\t\t\t\tprovide a lot of information - as a result I need to use a lot\n\t\t\t\tof screen-shots.\n\n\nFigure 14: The cell\n\t\t\t\tcollection for the plan line.\n\n\n\n\nThe name of the plan appears in a single cell within the\noverall cell collection for the plan editor. This is cell is a cell\ncollection, this time a horizontal collection, of two sub-cells: a\nconstant and a property. (The editor editor indicates a vertical cell\ncollection by [/ and a horizontal collection by\n[\u003e.)\n\n\nFigure 15: A constant\n\t\t\t\tfor the word 'plan' in the plan line.\n\n\n\n\nThe constant is just the work plan. You can use constant\ncells to place any markers or hints into an editor. You also use blank\nconstant cells to do layout such as blank lines and indentation. The\ndelimiters in the plan editor (such as [/ and\n[\u003e) are also constants defined in the editor for editors.\n\n\n\nFigure 16: A property\n\t\t\t\tfor the name in the plan line.\n\n\n\n\nWe show the name of the plan with a property cell. The\n\t\t\t\tproperty can be any property on the concept that the editor is\n\t\t\t\tdefining for. Here I show I'm editing the property field on\n\t\t\t\tthe inspector and I have a pop up showing all the properties\n\t\t\t\ton the plan concept - in this case there's only one.\n\nThe blank lines in the editor are simple constant\n\t\t\t\tcells. The value and event lines involve sub-editors. I'll\n\t\t\t\tskip over the values row and dig into the events.\n\nThe event row is a cell in the vertical cell collection\n\t\t\t\tthat is itself a horizontal cell collection with two\n\t\t\t\tsub-cells: a blank constant cells and a ref node list cell\n\t\t\t\tmarked with (\u003e.\n\n\nFigure 17: The link cell\n\t\t\t\tfor events.\n\n\n\n\nA link node has a rather more complex inspector than the\n\t\t\t\tother nodes we've seen so far, but what interests us here is\n\t\t\t\ttwo bits of information. As you might guess from the name, ref\n\t\t\t\tnode list cells will list elements based on following a link\n\t\t\t\tin the schema. The editor tells which link to follow and that\n\t\t\t\tthe list should be made vertically. In  Figure 17 I'm showing the pop\n\t\t\t\tup for choosing the link (there actually is a choice this time)\n\t\t\t\tin the editor pane itself. I could also do it in the\n\t\t\t\tinspector. The editor pane shows the link name inside\n\t\t\t\t% delimiters.\n\nThis little example suggests an interesting question when\n\t\t\t\tyou define editors: should you edit using a separate inspector\n\t\t\t\tor directly in the editor pane itself? Keeping stuff out of\n\t\t\t\tthe editor pane allows you to get an overall structure in the\n\t\t\t\teditor pane and to better the see the relationship between the\n\t\t\t\teditor definition and the resulting use of the editor. However\n\t\t\t\tif you put everything in inspectors you're constantly digging\n\t\t\t\taround to see what what's the the cells. This is part of the\n\t\t\t\tjustification for terse markers (like the [/ and [\u003e). You can\n\t\t\t\tclick on the markers to see what they are in the inspector,\n\t\t\t\tbut as you get used to that particular editor you get used to\n\t\t\t\treading the editor pane directly. As you get used to it the\n\t\t\t\tterseness is helpful as it allows your eye to see more in less\n\t\t\t\tspace.\n\nYou could also image multiple editors for different\npurposes, some to suit people's experience, others just to suit\npeople's preferences. For example the intentional editor often allows\nyou to switch quickly between different projections according to your\npreferences. When editing nested tables like this, you can choose to\nswitch between nested tables (called boxy), a lisp like representation\n(lispy), or a tree view with properties (no cute name).  To edit\nconditional logic there is a C-like programming language view, or a\ntabular representation. This quick switching between projections is\nuseful because often you can see different aspects of a problem in\ndifferent projections, so you can often understand more from easy\nchanges between simple projections.\n\n\nBut let's get back to the example. We've seen that the plan\n\t\t\t\teditor has a cell that lists events vertically. How do we edit\n\t\t\t\tthose events? At this point we switch over the event\n\t\t\t\teditor. Our final tool will embed these event editors in the plan\n\t\t\t\teditor. \n\nBefore we look at the editor, let's refresh ourselves on\n\t\t\t\tthe schema for event.\n\n\nFigure 6: Schema for\n\t\t\t\tEvent\n\n\n\n\nHere's the definition for the event editor, just using\n\t\t\t\twhat's in the edit plane.\n\n\nFigure 19: Editor pane definition for\n\t\t\t\tevent editor.\n\n\n\n\nIf your eyes are getting use to the terse symbols, you\n\t\t\t\tshould be able to get most of this without using the\n\t\t\t\tinspector. Essentially we have a vertical cell collection with\n\t\t\t\ttwo elements. The bottom of the two is a ref node list cell to\n\t\t\t\tlist the posting rule temporal properties which will use the\n\t\t\t\teditor defined for that concept. The top cell however, shows\n\t\t\t\tsome stuff we haven't seen yet. \n\nThe top cell is a horizontal cell collection. It has two\n\t\t\t\tsub-cells. The left sub-cell is a constant cell with the word\n\t\t\t\t'event' - nothing new there. The new element is the second\n\t\t\t\tcell which is a ref node cell. Ref node cells are similar to\n\t\t\t\tref node list cells, but are for cases where the referred to\n\t\t\t\tlink is single valued - as it is here for the event type.\n\nThe ref node cell itself has two parts. The first indicates\n\t\t\t\twhich link to follow, in this case type. The\n\t\t\t\tsecond indicates which property of the target to display. This\n\t\t\t\tis an optional piece - had we left it out the event type would\n\t\t\t\trender using its regular editor. Here we are indicating that\n\t\t\t\trather than do that, we just want to render a single property:\n\t\t\t\tthe name of the type.\n\nNow lets look at the editor definition for posting rules.\nIn the example plan ( Figure 1, we\nsee that the editor shows the effective date of the rule followed by\nthe details of the rule itself. Here's the editor definition: \n\n\nFigure 20: Editor for\n\t\t\t\tPosting Rule Temporal Property\n\n\n\n\nThis time the root cell is a horizontal cell collection\n\t\t\t\twith three sub-cells: a ref node cell for the date, a constant\n\t\t\t\tcell for the \":\", and another ref node cell for the posting\n\t\t\t\trule itself. Both the date and posting rules are rendered with\n\t\t\t\ttheir own editor.\n\nThe last editor I'll show is the posting rule editor.\n\n\nFigure 21: Editor for\n\t\t\t\tPosting Rule\n\n\n\n\nHopefully by now this is almost familiar. The root is a\n\t\t\t\tvertical cell collection with two horizontal cell collections\n\t\t\t\tas sub-cells. The top cell has the constant \"amount:\" and a\n\t\t\t\tref node for the expression. The expression is rendered by the\n\t\t\t\teditor for expressions which is part of the formula\n\t\t\t\tlanguage. The bottom cell has the constant \"account:\" followed\n\t\t\t\tby a ref node for the account which shows the name property of\n\t\t\t\tthe account.\n\nDescribing an editor like this in text is awkward, at some\n\t\t\t\tpoint a screen cast of using the editor might be easier to\n\t\t\t\tfollow. The editor editor is a bit awkward to get used to. This is\n\t\t\t\tpartly because I'm not used to defining editors, partly its\n\t\t\t\tbecause more work is needed to make the editor editor\n\t\t\t\tusable. This is new territory so JetBrains is still learning\n\t\t\t\thow this kind of thing should work.\n\nThe important things to come out of this are that you need\n\t\t\t\ta lot of flexibility to define editors so they are as clean as\n\t\t\t\tthe final plan editor turns out to be. To provide this\n\t\t\t\tflexibility you end up with a complex editor for\n\t\t\t\teditors. Although there's probably much that can be done to\n\t\t\t\tmake these more usable, I suspect that it will still take some\n\t\t\t\teffort to define an editor that works well for a\n\t\t\t\tlanguage. However since the editor is closely integrated with\n\t\t\t\tthe other elements of a DSL, it's relatively easy to\n\t\t\t\texperiment and change editor definitions to explore the best editor.\n\nThe interplay here between the main edit window and the\n\t\t\t\tinspector reveals another point about editors for more complex\n\t\t\t\tDSLs such as an editor language. Rather than trying to get all\n\t\t\t\tyour editing through a single projection, it's often best to\n\t\t\t\tuse multiple projections that show different things. Here we\n\t\t\t\tsee the overall structure of the editor in the main editor\n\t\t\t\tpane, and lots of details in the inspector. When designing the\n\t\t\t\teditor, you can move different elements between different\n\t\t\t\tpanes. \n\nIn this case I can see that the third pane, showing the\nhierarchy of cells, would provide a useful third projection that would\ncomplement the inspector and wisiwigish main editor pane.\n\n\n\nDefining the Generator\n\nThe last part of the trio is to write the generator. In\n\t\t\t\tthis case we'll generate a java class that will create the\n\t\t\t\tappropriate objects using the framework we currently\n\t\t\t\thave. This plan builder class will create an instance of\n\t\t\t\tthe service agreement class for each each plan we've defined\n\t\t\t\tusing the DSL.\n\nThe code that we'll generate will look a little different\n\t\t\t\tto the java equivalent code we saw earlier. This is because of the\n\t\t\t\tway we're going to handle the calculation formulae. In the\n\t\t\t\tpure java version I used parameterized but limited formula\n\t\t\t\tclasses to set the formulae. In this version the formulae are\n\t\t\t\tsupplied by the formula language.\n\nHere's the edit pane projection of the generator definition\n\n\nFigure 22: The definition\n\t\t\t\tfor generation.\n\n\n\n\nHere's the code it generates: (I've added some line breaks\n\t\t\t\tto help format it for the web page.)\n\npackage postingrules;\n\n/*Generated by MPS*/\n\n\nimport postingrules.AgreementRegistry;\nimport postingrules.ServiceAgreement;\nimport postingrules.EventType;\nimport postingrules.AccountType;\nimport jetbrains.mps.formulaLanguage.api.MultiplyOperation;\nimport jetbrains.mps.formulaLanguage.api.DoubleConstant;\nimport jetbrains.mps.formulaLanguage.api.IfFunction;\nimport formulaAdapter.*;\nimport mf.*;\n\npublic class AgreementRegistryBuilder {\n  public void setUp(AgreementRegistry registry) {\n    registry.register(\"regular\", this.setUpRegular());\n    registry.register(\"lowPay\", this.setUpLowPay());\n  }\n  public ServiceAgreement setUpRegular() {\n    ServiceAgreement result = new ServiceAgreement();\n    result.registerValue(\"BASE_RATE\");\n    result.setValue(\"BASE_RATE\", 10.0, MfDate.PAST);\n    result.setValue(\"BASE_RATE\", 12.0, new MfDate(1999, 12, 1));\n    result.addPostingRule(\n      EventType.USAGE, \n      new PostingRule_Formula(AccountType.BASE_USAGE, true, \n        new MoneyAdapter(new MultiplyOperation(\n          new ValueDouble(\"BASE_RATE\"), new UsageDouble()),\n          Currency.USD)), \n      new MfDate(1999, 10, 1));\n    result.addPostingRule(\n      EventType.SERVICE_CALL, \n      new PostingRule_Formula(AccountType.SERVICE, true, \n        new MoneyAddOperation(\n          new MoneyMultiplyOperation(new FeeMoney(), new DoubleConstant(0.5)), \n          new MoneyConstant(10.0, Currency.USD))), \n      new MfDate(1999, 10, 1));\n    result.addPostingRule(\n      EventType.SERVICE_CALL, \n      new PostingRule_Formula(AccountType.SERVICE, true, \n        new MoneyAddOperation(\n          new MoneyMultiplyOperation(new FeeMoney(), new DoubleConstant(0.5)), \n          new MoneyConstant(15.0, Currency.USD))), \n      new MfDate(1999, 12, 1));\n    result.addPostingRule(\n      EventType.TAX, \n      new PostingRule_Formula(AccountType.TAX, false, \n        new MoneyMultiplyOperation(new FeeMoney(), new\n        DoubleConstant(0.055))), \n      new MfDate(1999, 10, 1));\n    return result;\n  }\n  public ServiceAgreement setUpLowPay() {\n    ServiceAgreement result = new ServiceAgreement();\n    result.registerValue(\"BASE_RATE\");\n    result.registerValue(\"REDUCED_RATE\");\n    result.registerValue(\"CAP\");\n    result.setValue(\"BASE_RATE\", 10.0, MfDate.PAST);\n    result.setValue(\"REDUCED_RATE\", 5.0, MfDate.PAST);\n    result.setValue(\"CAP\", new Quantity(50.0, Unit.KWH), MfDate.PAST);\n    result.setValue(\"CAP\", new Quantity(60.0, Unit.KWH), new MfDate(1999, 12, 1));\n    result.addPostingRule(\n      EventType.USAGE, \n      new PostingRule_Formula(AccountType.BASE_USAGE, true, \n        new IfFunction\u003cMoney\u003e(\n          new QuantityGreaterThenOperation(new UsageQuantity(), new ValueQuantity(\"CAP\")), \n          new MoneyAdapter(\n            new MultiplyOperation(new ValueDouble(\"BASE_RATE\"), new UsageDouble()), \n            Currency.USD), \n          new MoneyAdapter(\n            new MultiplyOperation(new ValueDouble(\"REDUCED_RATE\"), new\n            UsageDouble()), \n            Currency.USD))), \n      new MfDate(1999, 10, 1));\n    result.addPostingRule(\n      EventType.SERVICE_CALL, \n      new PostingRule_Formula(AccountType.SERVICE, true, \n        new MoneyConstant(10.0, Currency.USD)), \n      new MfDate(1999, 10, 1));\n    result.addPostingRule(\n      EventType.TAX, \n      new PostingRule_Formula(AccountType.TAX, false, \n        new MoneyMultiplyOperation(new FeeMoney(), new\n        DoubleConstant(0.055))), \n      new MfDate(1999, 10, 1));\n    return result;\n  }\n}\n\n\nAs usual I'll pick some bits of the generation to walk through,\n\t\t\t\twithout going into all of it. In particular the generated code\n\t\t\t\tfrom the formula is a rather ugly interpreter formula. This\n\t\t\t\tneeds to be cleaned up and we hope to do that in the near\n\t\t\t\tfuture.\n\nAs with any template language, MPS's generator language\nallows you to write the class in template form with parameter\nreferences. One big difference with a language workbench is that\nyou're using a projectional editor to define the template. So we can\ncreate a projectional editor for java class generation that knows\nabout java syntax and uses this information to help you in your\ntemplate generation. Here you see the generator editor has supplied\nmarkers for the various kinds of elements we see in java programs.\nThis one only has methods so these others are unused.\n\nMPS's generator language uses two kinds of parameter\n\t\t\t\treferences: property macros (marked with $) and\n\t\t\t\tnode macros ($$). Property macros interrogate the\n\t\t\t\tabstract representation and return a string to be inserted\n\t\t\t\tinto the templated output. Node macros interrogate the\n\t\t\t\tabstract representation and return further nodes for more\n\t\t\t\tprocessing. Typically you'll use node macros to handle the\n\t\t\t\tequivalent of loops in other templating systems.\n\nBoth types of macro are\n\t\t\t\timplemented by java methods in supporting java classes. In\n\t\t\t\ttime the MPS team wants to replace java by a DSL that's\n\t\t\t\tdesigned to query the abstract syntax for generation, but for\n\t\t\t\tthe moment they use java code.\n\nThe property macro is shown with references like\n\t\t\t\t$[_registryBuilder_]. Selecting the $ allows you to see in the\n\t\t\t\tinspector what java method is invoked by the macro. \n\n\nFigure 23: Linking to a\n\t\t\t\tjava property  macro\n\n\n\n\nIntegration between MPS and JetBrains's IntelliJ Java IDE\n\t\t\t\tallows me to hit the traditional IntelliJ \u003cCTRL\u003e-B and go\n\t\t\t\tthe definition of the macro in Java\n\n  public static String propertyMacro_RegistryBuilder_ClassName(SemanticNode sourceNode,\n     SemanticNode templateNode, PropertyDeclaration property, \n     ITemplateGenerator generator) \n  {\n    return NameUtil.capitalize(generator.getSourceModel().getName()) +\n      \"RegistryBuilder\";\n  }\n\n\nAs you can see this is a pretty simple method. Essentially\n\t\t\t\tall it does is concatenate the name of the model that we're\n\t\t\t\tworking on with \"RegistryBuilder\" to synthesize the class name.\n\nThis kind of thing allows you to synthesize various strings\n\t\t\t\tto insert in the generated code. While you're in the method\n\t\t\t\tyou have access to various parts of the abstract\n\t\t\t\trepresentation: both of the agreement DSL and the generator\n\t\t\t\tDSL.\n\n\nsourceNode is the current node in the source language -\n\t\t\t\t\tin this case the agreement language.\n\ntemplateNode is the current node in the generator\n\t\t\t\t\tlanguage, in this case the current node from the generator\n\t\t\t\t\tdefinition for builders\n\nproperty is the current property to which we're applying\n\t\t\t\t\tthe macro\n\nproperty declaration is the declaration (from the\n\t\t\t\t\tschema) for this property.\n\ngenerator is the current generator instance - this links\n\t\t\t\t\tin with the current project and models.\n\n\nYou can see in the editor projection that this parameter\n\t\t\t\treference has a name in the editor projection:\n\t\t\t\t_registryBuilder_. This is a label that allows\n\t\t\t\tmultiple references in the editor. You can see an example of\n\t\t\t\tthis later in the template. Each agreement is built with a separate method\n\t\t\t\t(setUpRegular() and\n\t\t\t\tsetUpLowPay()). These need to be called from the\n\t\t\t\toverall setup method. So the names of these methods have to be\n\t\t\t\treferenced from both the method definition and the call. The\n\t\t\t\tlabel _setUp_plan_  allows us to do that. In\n\t\t\t\t Figure 22 you can\n\t\t\t\tsee the label both within the repeating lines of the setUp\n\t\t\t\tmethod and as the method name in the template that's generated\n\t\t\t\tfor each method. Indeed, since the template editor\n\t\t\t\tis a projectional editor, we can get pop menus to help us\n\t\t\t\tchoose these labels when we need them. Since the editor knows\n\t\t\t\twe are building a template for a java program, it can use this\n\t\t\t\tinformation to help us edit in the projection.\n\nThe second kind of macro we can see is a node macro. Node\n\t\t\t\tmacros appear in the editor as $$[more template\n\t\t\t\tcode]. The template code enclosed in the brackets is\n\t\t\t\tapplied to each node returned from the macro. Here's the\n\t\t\t\tscreen for the our agreement creation methods.\n\n\nFigure 24: Linking to a\n\t\t\t\tnode  macro\n\n\n\n\nThis links to the following java code.\n\n  public static List\u003cSemanticNode\u003e templateSourceQuery_Plans(SemanticNode parentSourceNode,\n     ITemplateGenerator generator) \n  {\n    List\u003cSemanticNode\u003e list = new LinkedList\u003cSemanticNode\u003e();\n    List\u003cSemanticNode\u003e roots = generator.getSourceModel().getRoots();\n    for (SemanticNode node : roots) {\n      if (node instanceof Plan) {\n        list.add(node);\n      }\n    }\n    return list;\n  }\n\n\nAs you see, while a property macro returns a string, a node\nquery returns a list of semantic nodes - in this case it walks through\nthe roots of the abstract representation and returns all the plan\nnodes there. The generator will then generate the enclosed defined\ncode for each plan. (In this way it acts rather like the looping directive in VTL).\n\nWhen you're inside a node macro, the enclosed template is\n\t\t\t\tapplied once for each node returned by the macro - setting the\n\t\t\t\tsourceNode argument to that node. So when we name the method\n\t\t\t\tlater on we can use the following bit of java.\n\n  public static String propertyMacro_Plan_SetUpMethod_Name(SemanticNode sourceNode, \n      SemanticNode templateNode, PropertyDeclaration property, \n      ITemplateGenerator generator) \n  {\n    Plan plan = (Plan) sourceNode;\n    return \"setUp\" + plan.getName();\n  }\n\n\nSince the source node is a plan node, and the plan's schema has\n\t\t\t\ta name which is a string, we can just use the name to generate\n\t\t\t\tthe name of the method.\n\nThe rest of the template works in essentially the same\n\t\t\t\tway. Either you obtain properties from your current source\n\t\t\t\tnode, or you use a node macro to get another node to work on.\n\nDefining template in MPS is really very similar to\n\t\t\t\ttraditional template\n\t\t\t\tbased approaches. Again we have an abstract\n\t\t\t\trepresentation that we query inserting the results in the\n\t\t\t\tgenerated code. The main difference visible from this example\n\t\t\t\tis that we are able to build projectional editors for\n\t\t\t\tdifferent kinds of templated output - a java class in this\n\t\t\t\tcase.\n\n\n\nSumming Up\n\nI hope this example gives you a feel of what it's like to use\na language workbench - even if it's still somewhat of an embryo. In\nmany ways this example is mostly lacking in the respect that it's\nstill rather like a traditional textual DSL. As I suggested in language workbench, I think the\nreally interesting DSLs will actually by quite different. But part of\nthe the nature of this work is that we can't really see what they'll\nlook like yet.\n\n\n\n","author":"Martin Fowler"},{"id":"47ca1ade-b6b5-409c-a5f8-cb730e53b14d","title":"APIs should not be copyrightable","link":{"Scheme":"https","Opaque":"","User":null,"Host":"martinfowler.com","Path":"/articles/copyright-api.html","RawPath":"","OmitHost":false,"ForceQuery":false,"RawQuery":"","Fragment":"","RawFragment":""},"text":"\nLast month, the Electronic Frontier Foundation (EFF)\n    filed\n    an amicus brief with the Supreme Court of the United States, asking the\n    justices to review an earlier lower court decision that allows\n    APIs (Application Programming Interfaces) to be copyrightable. I'm\n    one of the 77 software professionals who signed the brief,\n    although rather intimidated by a group that includes Abelson \u0026\n    Sussman, Aho \u0026 Ullman, Josh Bloch, Fred Brooks, Vint Cerf,\n    Peter Deutsch, Mitch Kapor, Alan Kay, Brian Kernighan, Barbara\n    Liskov, Guido van Rossum, Bruce Schneier, and Ken Thompson.\n\n\n\n\n\n\n\nThe original lawsuit was brought by Oracle against Google,\n    claiming that Oracle held a copyright on the Java APIs, and that\n    Google infringed these APIs when they built Android. My support in\n    this brief has nothing to do with the details of the dispute\n    between these two tech giants, but everything to do with the\n    question of how intellectual property law should apply to\n    software, particularly software interfaces.\n\nI'm not part of the thinking that asserts that nothing in\n    software should be intellectual property. While I do think that\n    software patents are inherently\n    broken, copyright is a good mechanism to allow software\n    authors to have some degree of control over of what happens with their hard work.\n\nSoftware has always been a tricky source of income, because\n    it's trivial to copy. Copyright provides a legal basis to control at least\n    some copying. Without something like this, it\n    becomes very hard for someone to work on creating things and still\n    be able to pay the mortgage. While we all like free stuff, I think\n    it's only fair to give people the chance to earn a living from the\n    work they do.\n\nBut any intellectual property mechanism has to balance this\n    benefit with the danger that excessive intellectual property\n    restrictions can impede further innovation, whether that be\n    extending an invention, or reimagining a creative work. As a\n    result, patent and copyright regimes have some form of limitation\n    built in. One limitation is one of time: patents and copyrights\n    expire (although the Mickey Mouse\n    discontinuity is threatening that).\n\nInterfaces are how things plug together. An example from the\n    physical world is cameras with interchangeable lenses. Many camera\n    makers don't encourage other companies to make lenses for their\n    cameras, but such third-party companies can reverse-engineer how\n    the interface works and build a lens that will mount on a camera.\n    We regularly see this happen with third-party parts providers -\n    and these third parties do a great deal to provide lower costs and\n    features that the main company doesn't support. I used a Sigma\n    lens with my Canon camera because Canon didn't (at the time)\n    make an 18-200mm lens. I've bought third party batteries for\n    cameras because they're cheaper. Similarly I've repaired my car with third party\n    parts again to lower costs or get an audio system that better\n    matched my needs.\n\nSoftware interfaces are much the same, and the ability to\n    extend open interfaces, or reverse-engineer interfaces, has played\n    a big role in advancing software systems. Open interfaces were a\n    vital part of allowing the growth of the internet, nobody has to\n    pay a copyright licence to build a HTTP server, nor to connect to\n    one. The growth of Unix-style operating systems relied greatly on\n    the fact that although much of the source code for AT\u0026T's Unix\n    was copyrighted, the interfaces were not. This allowed offshoots\n    such as BSD and Linux to follow Unix's interfaces, which helped\n    these open-source systems to get traction by making it easier for\n    programs built on top of Unix to interact with new\n    implementations.\n\n\nA picture is worth a 1000 words, so here's a picture of some books written by signatories of the EFF amicus brief -- Josh Bloch\n\n\nThe story of SMB and Samba is a good example of how\n    non-copyrightable APIs spurred competition. When Windows became a\n    dominent desktop operating system, its SMB protocol dominated\n    simple networks. If non-windows computers wanted to communicate\n    effectively with the dominant windows platform, they needed to\n    talk to SMB. Microsoft didn't provide any documentation to help\n    competitors do this, since an inability to communicate with SMB\n    was a barrier to their competitors. However, Andrew Tridgell was\n    able to deduce the specification for SMB and build an\n    implementation for Unix, called Samba. By using Samba non-windows\n    computers could collaborate on a network, thus encouraging the\n    competition from Mac and Linux based systems. A similar story\n    happened years before with the IBM BIOS, which was\n    reverse-engineered by competitors.\n\nThe power of a free-market system comes from competition, the\n    notion that if I can find a way to bake bread that's either\n    cheaper or tastier than my local bakers, I can start a bakery and\n    compete with them. Over time my small bakery can grow and compete\n    with the largest bakers. For this to work, it's vital that we\n    construct the market so that existing players that dominate the\n    market cannot build barriers to prevent new people coming in with\n    innovations to reduce cost or improve quality. \n\nSoftware interfaces are critical points for this with software.\n    By keeping interfaces open, we encourage a competitive\n    marketplace of software systems that encourage innovation to\n    provide more features and reduce costs. Closing this off will\n    lead to incompatible islands of computer systems, unable to\n    communicate.\n\nSuch islands of incompatibility present a considerable barrier\n    to new competitors, and are bad for that reason alone. But it's\n    they are bad for users too. Users value software\n    that can work together, and even if the various vendors of\n    software aren't interested in communication, we should encourage\n    other providers to step in and fill the gaps. Tying systems\n    together requires open interfaces, so that integrators can safely\n    implement an interface in order to create communication links. We\n    value standard connectors in the physical world, and while\n    software connections are often too varied for everything to be\n    standardized, we shouldn't use copyright law to add further hurdles.\n\nThe need to implement interfaces also goes much deeper than\n    this. As programmers we often have to implement interfaces defined\n    outside our code base in order to do our jobs. It's common to have\n    to modify software that was written with one library in mind to\n    work with another - a useful way to do this is to write adapters that implement the interface of the\n    first library by forwarding the second. Implementing interfaces is\n    also vital in testing, as it allows you to create Test Doubles.\n    \n\nSo for the sake of our ability to write programs properly, our\n    users' desire to have software work together, and for society's\n    desire for free markets that spur competition — copyright should\n    not be used for APIs.\n\n\nAddendum: The Supreme Court Decides\n\n\n\n        BREAKING: In major copyright battle between tech giants, SCOTUS sides w/\n        Google over Oracle, finding that Google didnt commit copyright\n        infringement when it reused lines of code in its Android operating\n        system.\n      -- SCOTUSblog\n\n\nThe case made its way to the Supreme Court, who on April 5th 2021\n      ruled in favor of Google. In this ruling they didn't address whether APIs\n      were copyrightable, rather they rules that Google's action of\n      reimplementing an API was Fair Use.\n\nI didn't consider Fair Use in the above article, but it seems to me to\n      be an effective way to frame the situation, and treating APIs in this way\n      avoids the problems that stem from copyrighting APIs.\n\nThe four page summary at the beginning of the\n      opinion is a pretty clear explanation of the logic of the argument.\n\n\n\n","author":"Martin Fowler"},{"id":"333cbad0-a23b-4a87-ac7d-999bafcc3f46","title":"Access Modifier","link":{"Scheme":"https","Opaque":"","User":null,"Host":"martinfowler.com","Path":"/bliki/AccessModifier.html","RawPath":"","OmitHost":false,"ForceQuery":false,"RawQuery":"","Fragment":"","RawFragment":""},"text":"\nObject-oriented languages divide a program into modules called\nclasses. Each class contains features, which consist of data (fields)\nand methods. (Not all languages use these terms, but they'll do for\nthis.) Languages have various rules about what other classes can\naccess the features of a class, these are often based on access\nmodifiers that apply to a class.\n\n\nC++ Choice\n\n\nProbably the most influential set of access modifiers started with\nC++, which has three.\n\n\npublic: Any class can access the features\n\nprotected: Any subclass can access the feature\n\nprivate: No other class can access the feature\n\n\nA class can also give access to another class or method with the\nfriend keyword - hence the comment that in C++ friends can touch\neach others' private parts.\n\n\n\nJava\n\n\nJava based itself on C++. It added the notion of package to the\nlanguage, and this influenced behavior.\n\n\npublic: Any class\n\n(package): (The default, and doesn't use a keyword in\nthe code) Any class in the same package\n\nprotected: Any subclass or class in the same package\n\nprivate: No other class\n\n\nNotice the subtle distinction between Java's protected and C++'s\nprotected (just to keep things confusing.)\n\n\n\nC#\n\nC# also is based on the C++ model\n\n\npublic: Any class\n\ninternal: Any class in the same assembly (default for\n\t\t\tmethods and classes, but may be specified)\n\nprotected: Any subclass\n\nprotected internal: Any subclass or class in the same assembly\n\nprivate: No other class (default for fields)\n\n\nIn C# an assembly is a physical unit of composition - equivalent to a\ndll, jar, or binary. C# also has logical units (namespaces) that are\nsimilar to java packages, but they don't play in access modifiers.\n\n\n\nSmalltalk\n\nSmalltalk is often considered to be the purest OO language, and\npredates C++, Java, and C#. It didn't use keywords to control access,\nbut used a basic policy. Smalltalkers would say that fields were\nprivate and methods were public.\n\nHowever the private fields don't really mean the same as what they\nmean in C++ based languages. In C++ et al access is thought of as\ntextual scope. Consider an example with a class Programmer which is a\nsubclass of class Person with two instances: Martin and Kent. In C++\nsince both instances are of the same class then Martin has access to\nthe private features of Kent. In Smalltalk's world view access is\nbased on objects, so since Martin and Kent are different objects\nMartin has no business getting at Kent's fields. But again, since\neverything is object based Martin can get at all his fields even if\nthey were declared in the Person class. So data in Smalltalk is closer\nto protected than private, although the object scope makes things\ndifferent in any case.\n\n\n\nAccess control does not control access\n\nIf you have a field that's private it means no other class can get at\nit. Wrong! If you really want to you can subvert the access control\nmechanisms in almost any language. Usually the way through is via\nreflection. The rationale is that debuggers and other system tools\noften need to see private data, so usually the reflection interfaces\nallow you to do this. \n\nC++ doesn't have this kind of reflection, but there you can just use\ndirect memory manipulation since C++ is fundamentally open memory.\n\nThe point of access control is not to prevent\n\t\taccess, but more to signal that the class prefers to keep some\n\t\tthings to itself. Using access modifiers, like so many things in\n\t\tprogramming, is primarily about communication.\n\n\n\nPublished Methods\n\nI've argued that there is really room for another access type:\nPublishedInterface. I think there is a\nfundamental difference between features you expose to other classes\nwithin your project team and those you expose to other teams (such as\nin an API). These published features are a subset of public features\nand have to be treated differently, so much so that I believe that the\ndistinction between published and public is more important than that\nbetween public and private..\n\n","author":"Martin Fowler"},{"id":"ba7859fa-3fcf-4af0-9852-64a5272374e2","title":"Accounting Patterns","link":{"Scheme":"https","Opaque":"","User":null,"Host":"martinfowler.com","Path":"/apsupp/accounting.pdf","RawPath":"","OmitHost":false,"ForceQuery":false,"RawQuery":"","Fragment":"","RawFragment":""},"text":"","author":"Martin Fowler"}]
